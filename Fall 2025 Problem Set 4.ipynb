{"cells":[{"cell_type":"markdown","metadata":{"id":"Y7ttBYeclcLI"},"source":["# Problem Set 4 (PSet 4): Federated Learning and Text-to-Image Generation\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mQGm6JtgehIq"},"source":["> Harvard CS 2420: Computing at Scale (Fall 2025)\n","\n",">\n","> Instructor: Professor HT Kung\n"]},{"cell_type":"markdown","source":["Team #:\n","\n","Team Members:\n","- Member 1\n","- Member 2\n","- Member 3\n","- Member 4"],"metadata":{"id":"524YPr71hzuX"}},{"cell_type":"markdown","metadata":{"id":"BRdPkqn04KqH"},"source":["### **Assignment Instructions**\n","\n","**Please start this assignment early.**\n","\n","Please read the following instructions carefully before starting the assignment and again before submitting your work:\n","\n","* Please enter your team number and the names of all team members in the text cell above.\n","* We expect this assignment to take more time. There is a more significant programming element involved, and more training time is required for the models. **Again, we suggest you start right away.**\n","* The assignment consists of two parts: **this Google Colab file** (an `.ipynb `file) and a **LaTeX answer template** (available on Canvas).\n","* The Google Colab contains all assignment instructions and *Code Cells* that you will use to implement the programming components of the assignment in Python. **(Note: if there are any wording differences between the LaTeX template and this Colab file, the Colab file is authoritative and takes precedence.)**\n","* We provide a significant amount of the code to make it easier to get started. In the *Code Cells*, please add comments to explain the purpose of each line of code in your implementation. **You will not receive credit for implementations that are not well-documented.**\n","* <font color='red'>**Deliverables are highlighted in red**</font> in this Google Colab file. Use the LaTeX answer template to write down answers for these deliverables.\n","* Each group will **submit** both a PDF of your answers, any logs from generative AI, and your Google Colab file (`.ipynb` file) containing all completed *Code cells* to \"Problem Set 4\" on Canvas. Only one submission per group. Check your `.ipynb` file using this [tool](https://htmtopdf.herokuapp.com/ipynbviewer/) before submitting to ensure that you completed all *Code Cells* (including detailed comments).\n","* The assignment is due on **Wednesday, November 5, 2025 at 11:59 PM EST**.\n","* Each part you are asked to implement is relatively small in isolation, and should be easy to test. We strongly recommend you test each of these parts before training the large models as to not waste time training models with buggy implementations. For example, you should ensure that your sampling is being done correctly, otherwise the model will still train, but your results will not be correct. For a number of sections, we have provided checks you can run to ensure correctness prior to training a large model.\n","\n","-----\n","An outline of this assignment with point values and training estimates is given below. Note that these estimates represent a lower bound on the running time, assuming a correct implementation.\n","\n","1. **Exploring Federated Learning (FL)** [25 points] [Training Estimate: 2 hours]\n","\n","2. **Non-IID Federated Learning and Fairness** [30 points] [Training Estimate: 3 hours]\n","\n","3. **Quantization of Local Models for Reduced Communication Cost** [25 points] [Training Estimate: 3 hours]\n","\n","4. **Backdoor Attacks by Malicious Clients and Defenses** [15 points] [Training Estimate: 1.5 hours]\n","\n","5. **Fine-Tune a Large Pre-Trained Stable Diffusion Model to Learn a New Concept** [25 points] [Training Estimate 1-2 hours]\n","\n","6. **Training-Free Multi-Prompt Generation with Varied Resolutions (Inference-Time)** [30 points]\n"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","> ## üíæ **Remember to Save Your Outputs**\n",">\n","> Colab runtimes can **reset or disconnect** after installs, version changes, or idle time.  \n","> To prevent data loss, **save your results frequently** to Google Drive or download them.\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"giXH6WJTucqS"}},{"cell_type":"markdown","source":["## ‚öôÔ∏è Prerequisites\n","Please install the following libraries before you start this assignment"],"metadata":{"id":"JfSPp7vBtcLN"}},{"cell_type":"code","source":["# Code Cell: Setup & Prerequisites\n","\n","!pip install diffusers==0.30.3      # For working with diffusion models (version 0.30.3)\n","!pip install transformers==4.44.2   # Provides access to pre-trained models and tokenizers (version 4.44.2)\n","!pip install matplotlib             # For plotting and visualizing data\n","!pip install xformers==0.0.28       # Optimizes memory usage for transformer models (version 0.0.28)\n","!pip install accelerate==0.34.2\n","!pip install bitsandbytes           # for 8-bit optimizers and GPU memory efficiency\n","!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu12"],"metadata":{"id":"jMZ3lImTtbyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Code Cell: Check Installed Libraries\n","\n","import torch\n","import diffusers\n","import transformers\n","import accelerate\n","import xformers\n","\n","# Expected versions\n","expected = {\n","    \"PyTorch\": \"2.4.1+cu121\",\n","    \"Diffusers\": \"0.30.3\",\n","    \"Transformers\": \"4.44.2\",\n","    \"Accelerate\": \"0.34.2\",\n","    \"Xformers\": \"0.0.28\",\n","}\n","\n","# Print detected versions\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Diffusers version: {diffusers.__version__}\")\n","print(f\"Transformers version: {transformers.__version__}\")\n","print(f\"Accelerate version: {accelerate.__version__}\")\n","print(f\"Xformers version: {xformers.__version__}\")\n","\n","# Check if they match\n","if (\n","    torch.__version__ == expected[\"PyTorch\"]\n","    and diffusers.__version__ == expected[\"Diffusers\"]\n","    and transformers.__version__ == expected[\"Transformers\"]\n","    and accelerate.__version__ == expected[\"Accelerate\"]\n","    and xformers.__version__ == expected[\"Xformers\"]\n","):\n","    print(\"Passed ‚úÖ\")\n","else:\n","    print(\"Version mismatch ‚ùå\""],"metadata":{"id":"RPpSSANHwyLI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UvFA89jTuON"},"source":["---\n","\n","### **1. Exploring Federated Learning (FL)**\n","\n","---\n","We will be using a dataset (CIFAR-10) and CNN model (`ConvNet`) introduced in Programming Assignment 2. *Code Cell 1.1* creates the CIFAR-10 training and testing datasets. Additionally, it also contains the CNN (`ConvNet`)  that will be used throughout the assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9WL6HA_Lpe8"},"outputs":[],"source":["## Code Cell 1.1\n","\n","import time\n","import copy\n","import sys\n","from collections import OrderedDict\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","\n","\n","# Using CIFAR-10 again as in Assignment 2\n","# Load training data\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True,\n","                                        transform=transform_train)\n","\n","# Load testing data\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True,\n","                                       transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n","\n","\n","# Using same ConvNet as in Assignment 2\n","def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n","               padding=1):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n","                  bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","        )\n","\n","class ConvNet(nn.Module):\n","    def __init__(self):\n","        super(ConvNet, self).__init__()\n","        self.model = nn.Sequential(\n","            conv_block(3, 32),\n","            conv_block(32, 32),\n","            conv_block(32, 64, stride=2),\n","            conv_block(64, 64),\n","            conv_block(64, 64),\n","            conv_block(64, 128, stride=2),\n","            conv_block(128, 128),\n","            conv_block(128, 256),\n","            conv_block(256, 256),\n","            nn.AdaptiveAvgPool2d(1)\n","            )\n","\n","        self.classifier = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        h = self.model(x)\n","        B, C, _, _ = h.shape\n","        h = h.view(B, C)\n","        return self.classifier(h)\n"]},{"cell_type":"markdown","metadata":{"id":"wjiB2IYp7B25"},"source":["**Federated Learning Overview**\n","\n","Federated Learning (FL) distributes the task of training a deep neural network (such as our CNN `ConvNet`) across multiple client devices. Each client may have private data they do not want to share with a central server. Therefore, instead of transmitting data, clients perform training locally and send the updated model parameters (e.g., convolutional weights) to the server. The server averages these parameters across multiple clients to update the centralized model. Finally, after the centralized model has been updated, the server sends the new version of the model to all clients.\n","\n","The figure below depicts this Federated Learning paradigm (taken from [Towards Federated Learning at Scale: System Design](https://arxiv.org/pdf/1902.01046.pdf)). At the beginning of a training round in the selection phase, a percentage of devices (i.e., clients) agree to participate. By agreeing to participate, a client agrees to perform local training with its own dataset that resides on the device. During the configuration phase, the up-to-date centralized model is sent to the participating clients, which then perform local training. In the reporting phase, each client sends their own updated model (trained using local data) to the server for aggregation. Note that, in the figure, one of the clients fails to report back to the central server (either due to device or network failure). To simplify this assignment, we will assume this type of device/network failure is not possible.\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?id=1y8HAIxtNaZVLWetXHEzJ4UXWJ0yX_Jo0' />\n","</figure>\n","\n","\n","**Simulating Federated Learning**\n","\n","In this assignment, we will simulate this distributed Federated Learning environment on a single machine (i.e., a Colab instance). Each `device` will own a subset (or partition) of the dataset (e.g., 10% of the CIFAR-10 training set) and use it to train a local version of the model. The main difference between this simulated environment and a real system is the lack of networking between devices.\n","\n","You will use the `DatasetSplit` class in *Code Cell 1.2* to create subsets of the full training dataset. The `create_device` function creates a unique instance of `ConvNet`, an instance of the `DatasetSplit` dataloader, and an optimizer and scheduler for training. This function will be called multiple times (once per device) to create all the required device instances used for Federated Learning. The `train` and `test` functions are a modified version from Assignment 2 that take a device argument (the output from `create_device`. The batch size during training is set to 128 throughout the assignment. This is passed into the `create_device` function as a default parameter value (i.e., `batch_size=128`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSx1GxV2j0iI"},"outputs":[],"source":["## Code Cell 1.2\n","import copy\n","\n","class DatasetSplit(torch.utils.data.Dataset):\n","    def __init__(self, dataset, idxs):\n","        self.dataset = dataset\n","        self.idxs = [int(i) for i in idxs]\n","\n","    def __len__(self):\n","        return len(self.idxs)\n","\n","    def __getitem__(self, item):\n","        image, label = self.dataset[self.idxs[item]]\n","        return image, torch.tensor(label)\n","\n","def create_device(net, device_id, trainset, data_idxs, lr=0.1,\n","                  milestones=None, batch_size=128):\n","    if milestones == None:\n","        milestones = [25, 50, 75]\n","\n","    device_net = copy.deepcopy(net)\n","    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9)\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n","                                                     milestones=milestones,\n","                                                     gamma=0.1)\n","    device_trainset = DatasetSplit(trainset, data_idxs)\n","    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n","                                                     batch_size=batch_size,\n","                                                     shuffle=True)\n","    return {\n","        'net': device_net,\n","        'id': device_id,\n","        'dataloader': device_trainloader,\n","        'optimizer': optimizer,\n","        'scheduler': scheduler,\n","        'train_loss_tracker': [],\n","        'train_acc_tracker': [],\n","        'test_loss_tracker': [],\n","        'test_acc_tracker': [],\n","        }\n","\n","def train(epoch, device):\n","    net.train()\n","    device['net'].train()\n","    train_loss, correct, total = 0, 0, 0\n","    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n","        inputs, targets = inputs.cuda(), targets.cuda()\n","        device['optimizer'].zero_grad()\n","        outputs = device['net'](inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        device['optimizer'].step()\n","        train_loss += loss.item()\n","        device['train_loss_tracker'].append(loss.item())\n","        loss = train_loss / (batch_idx + 1)\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","        acc = 100. * correct / total\n","        dev_id = device['id']\n","        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' +\n","                         f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n","        sys.stdout.flush()\n","    device['train_acc_tracker'].append(acc)\n","    sys.stdout.flush()\n","\n","def test(epoch, device):\n","    net.eval()\n","    test_loss, correct, total = 0, 0, 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","            outputs = device['net'](inputs)\n","            loss = criterion(outputs, targets)\n","            test_loss += loss.item()\n","            device['test_loss_tracker'].append(loss.item())\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","            loss = test_loss / (batch_idx + 1)\n","            acc = 100.* correct / total\n","    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n","    sys.stdout.flush()\n","    acc = 100.*correct/total\n","    device['test_acc_tracker'].append(acc)"]},{"cell_type":"markdown","metadata":{"id":"PTEyxu91KNl-"},"source":["**Single Device Scenario**\n","\n","Before implementing Federated Learning, we will train a model for a single client device using only local data without sending updates to a central server. By doing this, the device is only able to look at a small percentage of the CIFAR-10 training set (10% in this case), and should perform poorly.\n","\n","---\n","<font color='red'>**PART 1.1:**</font> [5 points]\n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 1.3*, implement the function `iid_sampler` to generate **IID** (independent and identically distributed) samples from the CIFAR-10 training set. We will use this function to generate training subsets for multiple devices in **PART 1.2**.\n","2. In *Code Cell 1.4*, create a single device using the function `create_device`. This device should have 10% of the CIFAR-10 training set, obtained using `iid_sampler`.\n","3. Train the model for 1000 epochs using the parameters specified in *Code Cell 1.4* (similar to Assignment 2). The number of epochs is 10x greater due to the single device having only 10% of the data. Plot the test accuracy (`device['test_acc_tracker']`) over the epochs and comment on the classification accuracy compared to using 100% of the dataset as in Assignment 2. (50 words maximum)\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-t76gDhjr-Z"},"outputs":[],"source":["## Code Cell 1.3\n","\n","def iid_sampler(dataset, num_devices, data_pct):\n","    '''\n","    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n","    num_devices: integer number of devices to create subsets for\n","    data_pct: percentage of training samples to give each device\n","              e.g., 0.1 represents 10%\n","\n","    return: a dictionary of the following format:\n","      {\n","        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n","        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n","        ...\n","      }\n","\n","    iid (independent and identically distributed) means that the indexes\n","    should be drawn independently in a uniformly random fashion.\n","    '''\n","\n","    # total number of samples in the dataset\n","    total_samples = len(dataset)\n","\n","    # Part 1.1: Implement!\n","\n","\n","    return sampled\n"]},{"cell_type":"markdown","metadata":{"id":"ifc4z-2Q8ab2"},"source":["Now, perform training using a single device on a subset of the training dataset using your `iid_sampler`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLd3fFeS8VEN"},"outputs":[],"source":["## Code Cell 1.4\n","\n","data_pct = 0.1\n","epochs = 100 # Part 1.1: Change to 1000 epochs\n","num_devices = 1\n","device_pct = 0.1\n","net = ConvNet().cuda()\n","criterion = nn.CrossEntropyLoss()\n","milestones = [250, 500, 750]\n","\n","# Part 1.1: Implement the cifar iid_sampler to generate data_idxs for create_device\n","data_idxs = iid_sampler(trainset, num_devices, data_pct)\n","\n","# Part 1.1: Create the device\n","device = None # Implement this!\n","\n","# Part 1.1: Train the device model for 100 epochs and plot the result\n","# Standard Training Loop\n","start_time = time.time()\n","for epoch in range(epochs):\n","    train(epoch, device)\n","    # To speed up running time, only evaluate the test set every 10 epochs\n","    if epoch > 0 and epoch % 10 == 0:\n","        test(epoch, device)\n","    device['scheduler'].step()\n","\n","\n","total_time = time.time() - start_time\n","print('Total training time: {} seconds'.format(total_time))"]},{"cell_type":"markdown","metadata":{"id":"4waD2ZlQI4X2"},"source":["**Implementing Components for Federated Learning**\n","\n","In **PART 1.1**, you implemented `iid_sampler`, created a 10% subset of the CIFAR-10 training set, and used it to train a single client device model. Since the client only had a 10% subset of the full CIFAR-10 training set, it performed significantly worse than the same model trained on the entire training set.\n","\n","Federated Learning aims to improve the performance of these client devices by averaging the updates from multiple clients over the course of training. This way, a centralized server is able to be updated using the training data stored on local devices without having access to the training data.\n","By using more client devices, you will be able to leverage the entire dataset.\n","In a way, this simulates traditional gradient descent, but with additional epochs performed on each client before averaging, where each epoch uses mini-batches of size 128.\n","An additional benefit of federated learning is that the centralized server does not require large compute resources, as most of the training computation is performed on local devices.\n","This makes the training computation \"free\" for the centralized server, as the clients pay the compute cost on their local devices.\n","\n","---\n","<font color='red'>**PART 1.2:**</font> [10 points]\n","\n","Before implementing Federated Learning, you must implement two functions which will be used during the training process.\n","\n","The `average_weights` function takes in multiple device models, and computes the average for each model parameter across all models. This function will be called by the centralized server to aggregate the training performed by the end user devices. This averaging is done in 32-bit floating point (`float32`).\n","\n","The `get_devices_for_round` function will be used to simulate the device rejection phase shown earlier in the figure in the **Federated Learning Overview** section. This function will select a percentage of devices to participate in each training round.\n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 1.5*, implement `average_weights`. We have provided test code you can use to validate your implementation. This test code will also be useful for the full implementation of Federated Learning in **PART 1.3**.\n","2. In *Code Cell 1.5*, implement the `get_devices_for_round` function. Try multiple `device_pct` settings to ensure that it is working properly.\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6X0fTWKg6hBY"},"outputs":[],"source":["## Code Cell 1.5\n","import copy\n","\n","\n","def average_weights(devices):\n","    '''\n","    devices: a list of devices generated by create_devices\n","    Returns an the average of the weights.\n","    '''\n","    # Part 1.2: Implement!\n","    # Hint: device['net'].state_dict() will return an OrderedDict of all\n","    #       tensors in the model. Return the average of each tensor using\n","    #       and OrderedDict so that you can update the global model using\n","    #       device['net'].load_state_dict(w_avg), where w_avg is the\n","    #       averaged OrderedDict over all devices\n","\n","\n","def get_devices_for_round(devices, device_pct):\n","    '''\n","    '''\n","    # Part 1.2: Implement!\n","\n","# Test code for average_weights\n","# Hint: This test may be useful for Part 1.3!\n","class TestNetwork(nn.Module):\n","    '''\n","    A simple 2 layer MLP used for testing your average_weights implementation.\n","    '''\n","    def __init__(self):\n","        super(TestNetwork, self).__init__()\n","        self.layer1 = nn.Linear(2, 2)\n","        self.layer2 = nn.Linear(2, 4)\n","\n","    def forward(self, x):\n","        h = F.relu(self.layer1(x))\n","        return self.layer2(h)\n","\n","data_pct = 0.05\n","num_devices = 2\n","net = TestNetwork()\n","data_idxs = iid_sampler(trainset, num_devices, data_pct)\n","devices = [create_device(net, i, trainset, data_idxs[i])\n","           for i in range(num_devices)]\n","\n","# Fixed seeding to compare against precomputed correct_weight_averages below\n","torch.manual_seed(0)\n","devices[0]['net'].layer1.weight.data.normal_()\n","devices[0]['net'].layer1.bias.data.normal_()\n","devices[0]['net'].layer2.weight.data.normal_()\n","devices[0]['net'].layer2.bias.data.normal_()\n","devices[1]['net'].layer1.weight.data.normal_()\n","devices[1]['net'].layer1.bias.data.normal_()\n","devices[1]['net'].layer2.weight.data.normal_()\n","devices[1]['net'].layer2.bias.data.normal_()\n","\n","# Precomputed correct averages\n","correct_weight_averages = OrderedDict(\n","    [('layer1.weight', torch.tensor([[ 0.3245, -0.9013], [-0.9042,  1.0125]])),\n","     ('layer1.bias', torch.tensor([-0.0724, -0.3119])),\n","     ('layer2.weight', torch.tensor([[0.2976,  1.0509], [-1.0048, -0.5972],\n","                                     [-0.3088, -0.2682], [-0.1690, -0.1060]])),\n","     ('layer2.bias', torch.tensor([-0.4396,  0.3327, -1.3925,  0.3160]))\n","    ])\n","\n","# Computed weight averages\n","computed_weight_averages = average_weights(devices)\n","\n","mismatch_found = False\n","for correct, computed in zip(correct_weight_averages.items(),\n","                             computed_weight_averages.items()):\n","    if not torch.allclose(correct[1], computed[1], atol=1e-2):\n","        mismatch_found = True\n","        print('Mismatch in tensor:', correct[0])\n","\n","if not mismatch_found:\n","    print('Implementation output matches!')"]},{"cell_type":"markdown","metadata":{"id":"AgzNJfpM4kUj"},"source":["**Federated Learning Training**\n","\n","---\n","<font color='red'>**PART 1.3:**</font> [10 points]\n","\n","We will now run the federated learning in the IID setting using the functions you previously wrote in this section.\n","The parameters are given to you in the code.\n","You will use 100 rounds of federated learning updates.\n","For each round, each device that participates in a given round will complete 4 epochs of local training.\n","10% of devices should participate in each round, selected by the `get_devices_for_round` function you wrote previously.\n","Note that we use static initialization for the models between all parts of the assignment.\n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 1.6*, train a global model via federated learning.\n","Much of the code has been given to you, but you will need to fill in the parts using calls to the functions you wrote above.\n","\n","2. Graph the accuracy of the global model over 100 rounds.\n","Discuss the accuracy difference between the global model trained here and the individual local model you trained in **PART 1.1**. (50 words maximum)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8OZxTJK4m8S"},"outputs":[],"source":["## Code Cell 1.6\n","\n","# use these parameters\n","rounds = 10 # Part 1.3: Change to 100 epochs\n","local_epochs = 4\n","num_devices = 50\n","device_pct = 0.1\n","data_pct = 0.1\n","net = ConvNet().cuda()\n","criterion = nn.CrossEntropyLoss()\n","\n","data_idxs = iid_sampler(trainset, num_devices, data_pct)\n","\n","# Part 1.3: Implement device creation here\n","devices = None # Implement this!\n","\n","\n","## IID Federated Learning\n","start_time = time.time()\n","for round_num in range(rounds):\n","    # Part 1.3: Implement getting devices for each round here\n","\n","    print('Round: ', round_num)\n","    for device in round_devices:\n","        for local_epoch in range(local_epochs):\n","            train(local_epoch, device)\n","\n","    # Weight averaging\n","    w_avg = average_weights(round_devices)\n","\n","    for device in devices:\n","        device['net'].load_state_dict(w_avg)\n","        device['optimizer'].zero_grad()\n","        device['optimizer'].step()\n","        device['scheduler'].step()\n","\n","    # test accuracy after aggregation\n","    test(round_num, devices[0])\n","\n","\n","total_time = time.time() - start_time\n","print('Total training time: {} seconds'.format(total_time))"]},{"cell_type":"markdown","metadata":{"id":"0KNmDbPh5b0h"},"source":["---\n","\n","### **2. Non-IID Federated Learning and Fairness**\n","\n","---\n","**Overview**\n","\n","In **PART 1**, you implemented a Federated Learning pipeline that operated on IID data.\n","While this IID assumption may hold in some applications, it does not hold in many other settings.\n","For example, a group of similar users may have data that is fundamentally different from that of another group of users.\n","As a result, the aggregate data that federated learning operates on will be non-IID in nature.\n","\n","In **PART 2** of the assignment, you will explore using Federated Learning in a non-IID setting.\n","In this part of the assignment, you will create groups of devices such that the inter-group data is non-IID and the intra-group data is IID.\n","To do this, you will reimplement many of the functions you implemented in **PART 1** for this non-IID setting.\n","\n","For all experiments in this section, we assume there are three groups.\n","Each group is assigned a different subset of classes in the dataset (Group 0 is assigned data from classes 0-3, Group 1 from classes 4-6, and Group 2 from classes 7-9).\n","We also fix each group to contain 20 devices, although you will vary per-group participation rates in each round in **PART 2.4**."]},{"cell_type":"markdown","metadata":{"id":"-yUOGnkZCbp-"},"source":["---\n","**Non-IID Sampling**\n","\n","<font color='red'>**PART 2.1:**</font> [10 points]\n","\n","We will first start by implementing `noniid_group_sampler`, a new, non-IID group version of the `iid_sampler` you implemented in PART 1.  We will use this function to generate training subsets for multiple devices in PART 2.4.  \n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 2.1*, implement the `noniid_group_sampler` function to generate **non-IID** samples from the CIFAR-10 training set.\n","As input, the function should take the dataset and number of training samples each device should be assigned.\n","As in `iid_sampler` you implemented previously, the function should return a `dict` where each key is a device ID number and each value is a `set` of the indices of training samples in the dataset assigned to that device.\n","You may want to have the function return other data as well, depending on how you implement functions in later parts of the assignment.\n","We have provided the mapping that indicates which classes are mapped to each group.\n","Within a given group, you should sample the data in an IID fashion.\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VajsGz0wiL05"},"outputs":[],"source":["## Code Cell 2.1\n","\n","# creates noniid TRAINING datasets for each group\n","def noniid_group_sampler(dataset, num_items_per_device):\n","  '''\n","    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n","    num_devices: integer number of devices to create subsets for\n","    num_items_per_device: how many samples to assign to each device\n","\n","    return: a dictionary of the following format:\n","      {\n","        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n","        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n","        ...\n","      }\n","\n","  '''\n","\n","  # how many devices per non-iid group\n","  devices_per_group = [20, 20, 20]\n","\n","  # label assignment per group\n","  dict_group_classes = {}\n","  dict_group_classes[0] = [0,1,2,3]\n","  dict_group_classes[1] = [4,5,6]\n","  dict_group_classes[2] = [7,8,9]\n","\n","  # Part 2.1: Implement!\n","  pass"]},{"cell_type":"markdown","metadata":{"id":"EY1H2bOELzX5"},"source":["---\n","**Group-based Device Rejection**\n","\n","<font color='red'>**PART 2.2:**</font> [5 points]\n","\n","We will now implement `get_devices_for_round_GROUP`, a new group-based version of the `get_devices_for_round` you implemented in **PART 1**.\n","We will use this function in **PART 2.4** to simulate the device rejection phase shown earlier on a per-group basis.  \n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 2.2*, implement the `get_devices_for_round_GROUP` function to generate a list of devices that will participate in each round of federated learning.\n","The function should, at minimum, take as input 1) the list of all devices, and 2) how many devices from each group should participate in a given round.\n","It should return a list of devices that will participate in a given round.\n","You may want to add additional input parameters depending on your implementation strategy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akRqxN7mMjfa"},"outputs":[],"source":["## Code Cell 2.2\n","\n","# get which devices in each group should participate in a current round\n","# by explicitly saying number of each devices desired for each group\n","def get_devices_for_round_GROUP(devices, device_nums, user_group_idxs):\n","  # PART 2.2: Implement!\n","  # Assume first 20 are group 0, second 20 are group 1, third 20 are group 2\n","  pass"]},{"cell_type":"markdown","metadata":{"id":"JBHBZVehNZNI"},"source":["---\n","**Group-based Testing**\n","\n","<font color='red'>**PART 2.3:**</font> [5 points]\n","\n","We will now implement the testing functions needed to evaluate the global model learned via Federated Learning on a per-group basis.  This will require two functions:\n","\n","\n","\n","* `cifar_noniid_group_test` divides the test dataset into three subsets, one subset for each group.\n","* `test_group` gets per-group classification accuracy for the global model.\n","You will likely want to start with the `test` function from **Code Cell 1.2** and modify it to work on a per-group basis.\n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 2.3*, implement the `cifar_noniid_group_test` function to create a test dataset for each group.\n","It should take the full CIFAR-10 test dataset as input, and return a `dict` where each key is a group ID, and each value is a `set` of the indices for all test samples for that group.\n","\n","2.  In *Code Cell 2.3*, implement the `test_group` function to output the per-group classification accuracy of the global model.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99S3opJONpeW"},"outputs":[],"source":["## Code Cell 2.3\n","\n","# creates noniid TEST datasets for each group\n","def cifar_noniid_group_test(dataset):\n","\n","  dict_group_classes = {}\n","  dict_group_classes[0] = [0,1,2,3]\n","  dict_group_classes[1] = [4,5,6]\n","  dict_group_classes[2] = [7,8,9]\n","\n","  # Part 2.3: Implement!\n","  pass\n","\n","# gets per-group accuracy of global model\n","def test_group(epoch, device, group_idxs_dict):\n","\n","    # Part 2.3: Implement!\n","    # Hint: refer to test function in PART 1\n","    # Hint: check https://pytorch.org/docs/stable/data.html?highlight=subset#torch.utils.data.Subset\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"CtkYyNF5Oo9J"},"source":["---\n","**Federated Learning Results in Non-IID Setting**\n","\n","<font color='red'>**PART 2.4:**</font> [10 points]\n","\n","We will now run federated learning in the non-IID setting using the functions you previously wrote.\n","We will examine two different scenarios.\n","\n","**Fair Device Participation:** run federated learning on the CIFAR-10 dataset with three groups.\n","Each group should have exactly one device participate in each round.\n","\n","**Unfair Device Participation:** run federated learning on the CIFAR-10 dataset with three groups.\n","Group 0 should have five devices participate in each round, and Groups 1 and 2 should each only have one device participate in each round.\n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 2.4*, train a global model via federated learning for the group-based non-IID setting.  Much of the code has been given to you, but you will need to fill in the parts using calls to the group-based, non-iid functions you wrote above.\n","(Hint: you will likely be able to re-use parts of the code you wrote in **Part 1.3**.)\n","\n","2. Graph the per-group test accuracy over 100 rounds for the **Fair Device Participation** scenario. Each group should have its own line in the graph.\n","\n","3.  Graph the per-group test accuracy over 100 rounds in the **Unfair Device Participation** scenario. Each group should have its own line in the graph.\n","\n","4.  Describe the differences you see between the two scenarios. How can you explain what you are seeing?  (100 words maximum)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eia5zgWx6ew3"},"outputs":[],"source":["## Code Cell 2.4\n","\n","rounds = 100\n","local_epochs = 1\n","num_items_per_device = 5000\n","device_nums = [5, 1, 1]\n","net = ConvNet().cuda()\n","criterion = nn.CrossEntropyLoss()\n","milestones=[250, 500, 750]\n","\n","# Part 2.4: Implement non-iid sampling\n","data_idxs = noniid_group_sampler(trainset, num_items_per_device)\n","\n","# Part 2.4: Implement device creation here\n","devices = [] # Implement this!\n","\n","test_idxs = cifar_noniid_group_test(testset)\n","## Non-IID Federated Learning\n","start_time = time.time()\n","for round_num in range(rounds):\n","\n","    # Get devices for each round\n","    round_devices = get_devices_for_round_GROUP(devices, device_nums, None)\n","\n","    print('Round: ', round_num)\n","    for device in round_devices:\n","        for local_epoch in range(local_epochs):\n","            train(local_epoch, device)\n","\n","    # Weight averaging\n","    w_avg = average_weights(round_devices)\n","\n","    for device in devices:\n","        device['net'].load_state_dict(w_avg)\n","        device['optimizer'].zero_grad()\n","        device['optimizer'].step()\n","        device['scheduler'].step()\n","\n","    # Test accuracy\n","    test_group(round_num, device, test_idxs)\n","\n","total_time = time.time() - start_time\n","print('Total training time: {} seconds'.format(total_time))"]},{"cell_type":"markdown","metadata":{"id":"mQB2URXWqB6m"},"source":["----\n","### **3. Quantization of Local Models for Reduced Communication Cost**\n","-----\n","Quantization refers to the process of reducing the number of bits used to represent a number. In the context of deep learning, the predominant numerical format used in research and deployment has been 32-bit floating-point ([IEEE 754 Format](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)).\n","However, the desire for reduced model size and computation has led to research on using fewer bits to represent numbers in deep learning models.\n","This can impact several aspects of the pipeline, including computation, communication, and storage requirements.\n","For example, in the context of federated learning, quantizing a client model from full (32-bit) precision to 8-bit precision will reduce the model size by ~4√ó.\n","Furthermore, because the model size is reduced, the communication required for uploading a client model is also reduced by ~4√ó as well.\n","\n","However, this quantization comes with trade-offs.\n","To see this, consider a full precision representation (32-bit floating point). This representation has a large dynamic range (from $-3.4\\times 10^{38}$ to $+3.4\\times10^{38}$) and high precision (about $7$ decimal digits).\n","As a result, a full precision number can be seen as continuous data.\n","In contrast, $n$-bit fixed-point representations are limited to $2^n$ discrete values.\n","$n$-bit quantization generally refers to projecting a full precision weight to one of these $2^n$ discrete values by finding its nearest neighbor.  \n","\n","--------\n","<font color='red'>**PART 3.1:**</font> [5 points]\n","\n","In this part, we will write a function to project full-precision numbers into $n$-bit fixed-point numbers.\n","For example, suppose we want to project full-precision numbers in the range of $[0, 1]$ into an 8-bit fixed point representation, $\\frac{1}{2^8-1}\\times(0, 1, 2, 3, \\dots, 253, 254,255)$, where $\\frac{1}{2^8-1}$ is the **scale factor** of the 8-bit fixed-point representation.\n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 3.1*, implement a function that converts full-precision numbers in the range $[0, 1]$ into $n$-bit fixed-point numbers.\n","If your implementation is correct, it should return *'Output of Quantization Matches!'*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1D9azF-qBCF"},"outputs":[],"source":["## Code Cell 3.1\n","\n","def quantizer(input, nbit):\n","    '''\n","    input: full precision tensor in the range [0, 1]\n","    return: quantized tensor\n","    '''\n","    # Part 3.1: Implement!\n","    # Hint: torch.round\n","    return input\n","\n","\n","# Test Code\n","test_data = torch.tensor([i/11 for i in range(11)])\n","\n","# ground truth results of 4-bit quantization\n","ground_truth = torch.tensor([0.0000, 0.0667, 0.2000, 0.2667, 0.3333, 0.4667,\n","                             0.5333, 0.6667, 0.7333, 0.8000, 0.9333])\n","\n","# output of your quantization function\n","quantizer_output = quantizer(test_data, 4)\n","\n","if torch.allclose(quantizer_output, ground_truth, atol=1e-04):\n","    print('Output of Quantization Matches!')\n","else:\n","    print('Output of Quantization DOES NOT Match!')"]},{"cell_type":"markdown","metadata":{"id":"BrvX-UzbqY4J"},"source":["**Quantize Weights of Neural Networks**\n","\n","The quantizer in **PART 3.1** will quantize any full-precision number in the range of $[0, 1]$ into an $n$-bit fixed-point number.\n","However, a weight $w$ in a neural network is not necessarily in the range $[0, 1]$.\n","\n","To use the quantizer in **PART 3.1**, we will first use a scaling function to transform weights into the range of $[0 ,1]$:\n","$$\\tilde{w} = \\frac{w}{2 \\cdot \\max(|w|)} + \\frac{1}{2}$$\n","where $2 \\cdot \\max(|w|)$ is the **adaptive scale**.\n","\n","Then, we quantize the transformed weights:\n","$$\\hat{w} = \\text{quantizer}_{\\text{n-bit}}(\\tilde{w})$$\n","After quantization, a reverse scaling function can be applied on $\\hat{w}$ to recover the original scale:\n","\n","$$w_q = 2 \\cdot \\max(|w|) \\cdot \\left( \\hat{w}-\\frac{1}{2} \\right)$$\n","\n","Combining these three equations, the expression we will use to get the quantized weights $w_q$ is as follows:\n","$$w_q = 2 \\cdot \\max(|w|) \\cdot \\left[ \\text{quantizer}_{\\text{n-bit}} \\left( \\frac{w}{2\\max(|w|)} + \\frac{1}{2} \\right) - \\frac{1}{2} \\right]$$\n","\n","This equation is the **deterministic quantization function**.\n","\n","Following the method proposed by [DoReFa-Net](https://arxiv.org/abs/1606.06160), we enable *stochastic quantization* by adding extra noise $N(n) = \\frac{\\sigma}{2^n-1}$ to the transformed weights $\\tilde{w}$, where $\\sigma \\sim \\text{Uniform}(-0.5, 0.5)$ and $n$ is the number of bits.\n","Generally, including such extra noise will coax the model into exploring more of the loss surface, helping the model escape local minima and improve model generalizability.  \n","\n","The final **stochastic quantization function** we will use to quantize layers of local models is:\n","\n","$$w_q = 2 \\cdot \\max(|w|) \\cdot \\left[ \\text{quantizer}_{\\text{n-bit}} \\left( \\frac{w}{2 \\cdot \\max(|w|)} + \\frac{1}{2} + N(n) \\right) - \\frac{1}{2} \\right]$$\n","\n","\n","<font color='red'>**PART 3.2:**</font> [10 points]\n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 3.2*, implement `dorefa_g(w, nbit, adaptive_scale=None)` using the **stochastic quantization function** shown above. Again, if your implementation is correct, it should return *'Output of Quantization Matches!'*.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGIOhKnWqXXr"},"outputs":[],"source":["## Code Cell 3.2\n","\n","def quantize_model(model, nbit):\n","    '''\n","    Used in Code Cell 3.3 to quantize the ConvNet model\n","    '''\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","            m.weight.data, m.adaptive_scale = dorefa_g(m.weight, nbit)\n","            if m.bias is not None:\n","                m.bias.data,_ = dorefa_g(m.bias, nbit, m.adaptive_scale)\n","\n","def dorefa_g(w, nbit, adaptive_scale=None):\n","    '''\n","    w: a floating-point weight tensor to quantize\n","    nbit: the number of bits in the quantized representation\n","    adaptive_scale: the maximum scale value. if None, it is set to be the\n","                    absolute maximum value in w.\n","    '''\n","    if adaptive_scale is None:\n","        adaptive_scale = torch.max(torch.abs(w))\n","\n","    # Part 3.2: Implement based on stochastic quantization function above\n","\n","    # remove placeholder \"return w, adaptive_scale\" line below\n","    # after you implement\n","    return w, adaptive_scale\n","\n","\n","# Test Code\n","test_data = torch.tensor([i/11 for i in range(11)])\n","\n","# ground truth results of 4-bit quantization\n","ground_truth = torch.tensor([-0.0606, 0.0606, 0.1818, 0.3030, 0.3030, 0.4242,\n","                             0.5455, 0.5455, 0.7879, 0.7879, 0.9091])\n","\n","# output of your quantization function\n","torch.manual_seed(43)\n","quantizer_output, adaptive_scale = dorefa_g(test_data, 4)\n","\n","print(ground_truth)\n","print(quantizer_output)\n","if torch.allclose(quantizer_output, ground_truth, atol=1e-04):\n","    print('Output of Quantization Matches!')\n","else:\n","    print('Output of Quantization DOES NOT Match!')"]},{"cell_type":"markdown","metadata":{"id":"AaaBm5RcqmMx"},"source":["**Reduce the Communication Overhead with Quantization**\n","\n","We will now explore the performance impact of quantization on federated learning. We will use the IID setting from **PART 1**. You will use the same federated learning code, but will first quantize each local model with the `quantize_model` function you wrote above before uploading to the central server (*Line 27, Code Cell 3.3*).\n","\n","<font color='red'>**PART 3.3:**</font> [10 points]\n","\n","<font color='red'>**Deliverables**</font>\n","1. In *Code Cell 3.3*, run federated learning with the following two quantization settings (bit widths): `nbit=16` and `nbit=4`. Plot the accuracy of the global models over 100 rounds for the different bit widths: 32-bit (the full-precision baseline you ran previously), 16-bit, and 4-bit.  \n","2. Discuss the accuracy difference between the global models across the three different bit width settings: 32-bit, 16-bit, and 4-bit. (100 words maximum)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUyOuVCpqrTI"},"outputs":[],"source":["## Code Cell 3.3\n","\n","# Part 3.2: Train two settings with nbit=16 and nbit=4.\n","#           Compare against the floating-point performance\n","#           of the final FL model trained in Part 1.3.\n","nbit = 16\n","\n","rounds = 100\n","local_epochs = 4\n","num_devices = 50\n","device_pct = 0.1\n","data_pct = 0.1\n","net = ConvNet().cuda()\n","criterion = nn.CrossEntropyLoss()\n","\n","data_idxs = iid_sampler(trainset, num_devices, data_pct)\n","devices = [create_device(net, i, trainset, data_idxs[i])\n","           for i in range(num_devices)]\n","\n","## IID Federated Learning\n","start_time = time.time()\n","for round_num in range(rounds):\n","    # Part 3.3: Implement!\n","    # Hint: you can use your federated learning code from PART 1\n","    pass\n","\n","    for device in devices:\n","        device['net'].load_state_dict(w_avg)\n","        device['optimizer'].zero_grad()\n","        device['optimizer'].step()\n","        device['scheduler'].step()\n","\n","    # test accuracy after aggregation\n","    test(round_num, devices[0])\n","\n","\n","total_time = time.time() - start_time\n","print('Total training time: {} seconds'.format(total_time))"]},{"cell_type":"markdown","metadata":{"id":"ehxZ_h81CKXy"},"source":["---\n","\n","### **4. Gradient Inversion Attacks to Federated Learning**\n","\n","In this part, you will explore gradient inversion attacks which may break privacy in federated learning. In federated learning, each client receives the current global weights of the network and sends weights updates (gradients) based on local data. But how secure is sharing weights gradients? The [Deep Leakage](https://arxiv.org/pdf/1906.08935.pdf) paper shows it is possible to recover data given weights gradients.\n","To perform the attack, Deep Leakage first randomly generates a pair of \"dummy\" inputs and labels and then performs the usual forward and backwards computation.\n","After deriving the dummy gradients from the dummy data, instead of optimizing model weights as in typical training, Deep Leakage optimizes the dummy inputs and labels to minimize the distance between dummy gradients and real gradients. You may refer to the paper for more details."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPkQJuvpNQJp"},"outputs":[],"source":["## Code Cell 4.1\n","dst = torchvision.datasets.CIFAR100(\"./data\", download=True)\n","tp = transforms.Compose([\n","    transforms.Resize(32),\n","    transforms.CenterCrop(32),\n","    transforms.ToTensor()\n","])\n","tt = transforms.ToPILImage()\n","\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","print(\"Running on %s\" % device)\n","\n","def label_to_onehot(target, num_classes=100):\n","    target = torch.unsqueeze(target, 1)\n","    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n","    onehot_target.scatter_(1, target, 1)\n","    return onehot_target\n","\n","def cross_entropy_for_onehot(pred, target):\n","    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))\n","\n","\n","def weights_init(m):\n","    if hasattr(m, \"weight\"):\n","        m.weight.data.uniform_(-0.5, 0.5)\n","    if hasattr(m, \"bias\"):\n","        m.bias.data.uniform_(-0.5, 0.5)\n","\n","class LeNet(nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","        act = nn.Sigmoid\n","        self.body = nn.Sequential(\n","            nn.Conv2d(3, 12, kernel_size=5, padding=5//2, stride=2),\n","            act(),\n","            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=2),\n","            act(),\n","            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n","            act(),\n","            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n","            act(),\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Linear(768, 100)\n","        )\n","\n","    def forward(self, x):\n","        out = self.body(x)\n","        out = out.view(out.size(0), -1)\n","        # print(out.size())\n","        out = self.fc(out)\n","        return out\n","\n","net = LeNet().to(device)\n","\n","net.apply(weights_init)\n","criterion = cross_entropy_for_onehot\n","\n","######### one client #########\n","img_index = 25\n","gt_data = tp(dst[img_index][0]).to(device)\n","gt_data = gt_data.view(1, *gt_data.size())\n","gt_label = torch.Tensor([dst[img_index][1]]).long().to(device)\n","gt_label = gt_label.view(1, )\n","gt_onehot_label = label_to_onehot(gt_label, num_classes=100)\n","\n","plt.imshow(tt(gt_data[0].cpu()))\n","plt.title(\"Ground truth image\")\n","print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n","\n","# compute original gradient\n","out = net(gt_data)\n","y = criterion(out, gt_onehot_label)\n","dy_dx = torch.autograd.grad(y, net.parameters())\n","\n","\n","# share the gradients with the server\n","original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n","\n","\n","def defense_method(dy_dx, defense_strategy):\n","\n","  if defense_strategy=='none':\n","    # no defense\n","    return dy_dx\n","\n","  elif defense_strategy=='pruning':\n","    # PART 4.1: pruning\n","    return ...\n","\n","  elif defense_strategy=='quantization':\n","    # PART 4.1: quantization\n","    return ...\n","\n","  elif defense_strategy=='noise':\n","    # PART 4.1: noise injection\n","    return ...\n","\n","\n","original_dy_dx = defense_method(original_dy_dx, your_defense)\n","\n","######### Start attack ##########\n","\n","# generate dummy data and label\n","dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n","dummy_label = torch.randn(gt_onehot_label.size()).to(device).requires_grad_(True)\n","\n","plt.imshow(tt(dummy_data[0].cpu()))\n","plt.title(\"Dummy data\")\n","print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())\n","\n","optimizer = torch.optim.LBFGS([dummy_data, dummy_label] )\n","\n","history = []\n","for iters in range(300):\n","    def closure():\n","        optimizer.zero_grad()\n","\n","        pred = net(dummy_data)\n","        dummy_onehot_label = F.softmax(dummy_label, dim=-1)\n","        dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label\n","        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n","\n","        grad_diff = 0\n","        grad_count = 0\n","        for gx, gy in zip(dummy_dy_dx, original_dy_dx): # TODO: fix the variables here\n","            grad_diff += ((gx - gy) ** 2).sum()\n","            grad_count += gx.nelement()\n","        # grad_diff = grad_diff / grad_count * 1000\n","        grad_diff.backward()\n","\n","        return grad_diff\n","\n","    optimizer.step(closure)\n","    if iters % 10 == 0:\n","        current_loss = closure()\n","        print(iters, \"%.4f\" % current_loss.item())\n","    history.append(tt(dummy_data[0].cpu()))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViVYjdl_uYue"},"outputs":[],"source":["## Code Cell 4.2\n","plt.figure(figsize=(12, 8))\n","for i in range(30):\n","  plt.subplot(3, 10, i + 1)\n","  plt.imshow(history[i * 10])\n","  plt.title(\"iter=%d\" % (i * 10))\n","  plt.axis('off')\n","print(\"Dummy label is %d.\" % torch.argmax(dummy_label, dim=-1).item())"]},{"cell_type":"markdown","metadata":{"id":"Sc0OjDXzNXqo"},"source":["<font color='red'>**PART 4.1:**</font> [15 points]\n","\n","In this part, we conduct a gradient inversion attack on a ConvNet model and implement several defense strategies. You will run experiments under 4 different settings:\n","1. Baseline: original Deep Leakage without any defenses (we have provided this code in *Code Cell 4.1*)\n","2. Noise: defend against the attack by injecting Gaussian noise  $ùí©(0, 10^{-3})$ to the weights gradients\n","3. Pruning: defend against the attack by pruning gradients: use unstructured magnitude pruning to set 20% of the gradient values to zero.\n","4. Quantization: defend against the attack by performing 4-bit quantization (`nbit=4`) on the gradients with the function you implemented in **PART 3.2**.\n","\n","<font color='red'>**Deliverables**</font>\n","\n","1. Implement the three defense strategies (i.e., noise injection, pruning, and quantization) `defense_method` in *Code Cell 4.1*.\n","2. Run the 4 experiments (i.e., Baseline, Noise, Pruning, and Quantization). For each experiment, you should report: (1) the final inversion result as measured by $\\|\\text{image}-\\text{recovered_image}\\|_2$ and (2) the optimization process of inversion using *Code Cell 4.2*.\n","3. Compare and discuss the effectiveness of the 3 different defense strategies. (100 words maximum)\n"]},{"cell_type":"markdown","metadata":{"id":"30lu8LWXmg5j","outputId":"31d88507-be24-4df5-a090-8a5d00d52d12"},"source":["\n","---\n","\n","### **5. Fine-Tune a Large Pre-Trained Stable Diffusion Model to Learn a New Concept**\n","\n","In this part, you will fine-tune Stable Diffusion v1.5 (SD v1.5) so it adapts to a new subject using few images ($\\approx 5$). After fine-tuning, the model should generate images that depict your subject when prompted.\n","\n","#### What you‚Äôll learn\n","\n","* How to take a large, pre-trained text-to-image model and specialize it to a new concept with few-shot data (5 images).\n","* The end-to-end workflow: data collection, token selection, fine-tuning, and inference.\n","\n","#### Workflow\n","\n","1. **Data Collection.**\n","   Use *five images of HT* as input. Your goal is to fine-tune SD v1.5 on these few samples so the model learns a concept that is **not present** in the original training data.\n","\n","2. **Token Selection.**\n","   Choose a rare identifier token‚Äîhere we use `sks`‚Äîas in *DreamBooth: Fine-Tuning Text-to-Image Diffusion Models for Subject-Driven Generation* (Ruiz et al., 2023).\n","   You will bind this special token to *HT‚Äôs appearance/patterns*, so prompts like\n","   `a photo of sks person`\n","   refer to your specific subject.\n","\n","3. **Fine-Tuning Session (you implement this).**\n","   Fine-tune *a small set of layers* in the U-Net of SD v1.5 (see *Part 5.1* and *Code Cell: 5.17* ) to adapt the model while using less GPU memory than updating all U-Net layers.\n","\n","4. **Inference (Generate images with the new concept).**\n","   After fine-tuning, generate images that include *HT‚Äôs pattern* when prompted.\n"]},{"cell_type":"markdown","metadata":{"id":"lULr-MQn31EM"},"source":["**Installing the Necessary Libraries**\n","\n","Before running the code in this notebook, make sure you have all the required libraries installed. You can install them using the commands below:"]},{"cell_type":"markdown","source":["\n","> **‚öôÔ∏è Code Cell 5.1 ‚Äî Install Required Libraries**\n",">\n","> This cell installs all necessary libraries for the assignment (see **Code Cell: Setup & Prerequisites**).\n","\n","> - If you **have not** installed them yet, run **Code Cell: Setup & Prerequisites** first.\n","> - If you **already** ran it, you can **skip** this cell and proceed to the next one."],"metadata":{"id":"UAIO61Q_y-7a"}},{"cell_type":"markdown","metadata":{"id":"aWFHI5gZ31EN"},"source":["**Checking GPU Details**\n","\n","To make sure you're using a GPU and to check its details, you can run the following command:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSq4hdx5B8Ow"},"outputs":[],"source":["## Code Cell 5.2\n","\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"7kzBzlgxp9zq"},"source":["**Core Library Imports for Model Operations**\n","\n","Below are the libraries needed for this notebook, along with the specific versions used:\n","\n","- **PyTorch version:** 2.4.1+cu121\n","- **Diffusers version:** 0.30.3\n","- **Transformers version:** 4.44.2\n","- **Accelerate version**: 0.34.2  \n","\n","\n","**Computing Resource**\n","\n","- GPU: 1√ó NVIDIA Tesla T4 (15,360 MB VRAM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joDspX60GL_4"},"outputs":[],"source":["## Code Cell 5.3\n","\n","import torch\n","import diffusers\n","import transformers\n","import accelerate\n","\n","# Expected versions\n","expected = {\n","    \"PyTorch\": \"2.4.1+cu121\",\n","    \"Diffusers\": \"0.30.3\",\n","    \"Transformers\": \"4.44.2\",\n","    \"Accelerate\": \"0.34.2\",\n","}\n","\n","# Print detected versions\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Diffusers version: {diffusers.__version__}\")\n","print(f\"Transformers version: {transformers.__version__}\")\n","print(f\"Accelerate version: {accelerate.__version__}\")\n","\n","# Check if they match\n","if (\n","    torch.__version__ == expected[\"PyTorch\"]\n","    and diffusers.__version__ == expected[\"Diffusers\"]\n","    and transformers.__version__ == expected[\"Transformers\"]\n","    and accelerate.__version__ == expected[\"Accelerate\"]\n","):\n","    print(\"Passed ‚úÖ\")\n","else:\n","    print(\"Version mismatch ‚ùå\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7DEquKgGXVj"},"outputs":[],"source":["## Code Cell 5.4\n","\n","import requests\n","from io import BytesIO\n","from PIL import Image\n","import logging\n","import sys\n","import re\n","import math\n","\n","import bitsandbytes as bnb\n","from tqdm import tqdm\n","import itertools\n","\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from transformers import CLIPTextModel, CLIPTokenizer, get_scheduler\n","from diffusers import (\n","    AutoencoderKL, UNet2DConditionModel, DDPMScheduler,\n","    StableDiffusionPipeline, DPMSolverMultistepScheduler)\n","from accelerate import Accelerator\n","from accelerate.utils import set_seed\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","import torch.nn.functional as F\n","\n","import gc\n","from diffusers import StableDiffusionPipeline"]},{"cell_type":"markdown","metadata":{"id":"XLqKB7ovqvjQ"},"source":["**Model Selection: Stable Diffusion v1.5**\n","\n","We have selected **Stable Diffusion v1.5** for its balanced performance in image quality and resource efficiency.\n","For versions 1.5, the default resolution remains at $512 \\times 512$, making them more memory efficient.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RTY0qfsMGawu"},"outputs":[],"source":["## Code Cell 5.5\n","pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n","resolution = 512"]},{"cell_type":"markdown","metadata":{"id":"HEuefNJ4rRRx"},"source":["**Data Collection: Training Subject: HT‚Äôs images**\n","\n","Use five images of HT as your training subject. These images will be used to fine-tune SD v1.5.\n","\n","Run *Code Cell 5.6* to define `urls` list with HT's images (it lists the five image *URLs*).\n","\n","You will *download and prepare* these images in *Code Cell 5.7*, saving them to `data/assets/` before fine-tuning.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-oyNWKsNGd8c"},"outputs":[],"source":["## Code Cell 5.6\n","\n","urls = [\n","      \"https://drive.google.com/file/d/1T2ikMo95TEi0jjtA_eDQhUzVMZgueGbD/view?usp=sharing\",\n","      \"https://drive.google.com/file/d/1W2oczGxst7NzzzTMn4RL3xHRKE0GfZ6E/view?usp=sharing\",\n","      \"https://drive.google.com/file/d/1UTG1lKVchRRCciVJQh0Gi-8XrsVhGJet/view?usp=sharing\",\n","      \"https://drive.google.com/file/d/1s_Gy9Zhs6KSGzrI5J8tKWqyYgTpIhA5o/view?usp=sharing\",\n","      \"https://drive.google.com/file/d/1zHdNa7txsoqh08KI2FsWnI8sGLNCjBHg/view?usp=sharing\",\n","]"]},{"cell_type":"markdown","metadata":{"id":"j6rj3uQ9r51Y"},"source":["**üåê Download Images from a URL**\n","\n","Downloads an image from a URL and converts it to RGB format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7h6jiBjpaQs"},"outputs":[],"source":["## Code Cell 5.7\n","def _extract_gdrive_file_id(url: str) -> str | None:\n","    # Matches: /file/d/<ID>/view ... or ... open?id=<ID>\n","    m = re.search(r\"/file/d/([a-zA-Z0-9_-]+)\", url)\n","    if m:\n","        return m.group(1)\n","    m = re.search(r\"[?&]id=([a-zA-Z0-9_-]+)\", url)\n","    if m:\n","        return m.group(1)\n","    return None\n","\n","def download_image(url: str, timeout: int = 10):\n","    file_id = _extract_gdrive_file_id(url)\n","    if not file_id:\n","        print(\"Error: could not parse Google Drive file id from URL.\")\n","        return None\n","\n","    session = requests.Session()\n","    base = \"https://drive.google.com/uc?export=download&id=\" + file_id\n","\n","    try:\n","        r = session.get(base, timeout=timeout, stream=True)\n","        r.raise_for_status()\n","\n","        if \"text/html\" in r.headers.get(\"Content-Type\", \"\").lower():\n","            m = re.search(r'confirm=([0-9A-Za-z_-]+)', r.text)\n","            if m:\n","                confirm = m.group(1)\n","                r = session.get(base + f\"&confirm={confirm}\", timeout=timeout, stream=True)\n","                r.raise_for_status()\n","\n","        # Load image from bytes\n","        data = r.content if not hasattr(r, \"raw\") else r.content\n","        img = Image.open(BytesIO(data)).convert(\"RGB\")\n","        return img\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error downloading image: {e}\")\n","    except (Image.UnidentifiedImageError, OSError) as e:\n","        print(f\"Error processing image: {e}\")\n","\n","    return None"]},{"cell_type":"markdown","metadata":{"id":"ow2MIsbspaQs"},"source":["**Visualization Function to Display Images**\n","\n","This function, `image_grid`, shows images in a grid using `matplotlib`:\n","\n","- **Inputs**:\n","  - `images`: List of images to display.\n","  - `rows`: Number of rows in the grid.\n","  - `cols`: Number of columns in the grid."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HrPy_-mTpaQs"},"outputs":[],"source":["## Code Cell 5.8\n","\n","def image_grid(images: list, rows: int = 1, cols: int = 2):\n","    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n","\n","    if not isinstance(axes, np.ndarray):\n","        axes = np.array([axes])\n","    axes = axes.ravel()\n","\n","    nslots = rows * cols\n","    for i in range(nslots):\n","        ax = axes[i]\n","        if i < len(images):\n","            ax.imshow(images[i])\n","            ax.axis(\"off\")\n","        else:\n","            ax.axis(\"off\")\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vRhGBWx9sBpj"},"source":["**Save Images to the `assets/` Folder**\n","\n","Saves images to the `assets/` folder using the `download_image` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WRH_Hv8GjYQ"},"outputs":[],"source":["## Code Cell 5.9\n","\n","from pathlib import Path\n","from PIL import Image\n","import os\n","\n","# Where images will be saved (downloaded/prepared in Code Cell 5.7)\n","subject_assets_path = Path(\"./assets/\")\n","subject_assets_path.mkdir(parents=True, exist_ok=True)  # Create folder if not exists\n","\n","# Download images: 'urls' is already defined in Code Cell 5.6\n","subject_images = [image for url in urls if (image := download_image(url)) is not None]\n","for idx, img in enumerate(subject_images):\n","    img.save(subject_assets_path.joinpath(f\"{idx}.jpeg\")) # Save each image\n","\n","# Quick visual check\n","image_grid(subject_images, rows=1, cols=len(subject_images))\n","print(f\"Saved {len(subject_images)} images to {subject_assets_path.resolve()}\")"]},{"cell_type":"markdown","metadata":{"id":"Hj2nkyROsMTG"},"source":["**The Logging Function**\n","\n","Records messages during code execution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzwOjC1eHyIj"},"outputs":[],"source":["## Code Cell 5.10\n","\n","def get_logger(name, log_file=None, level=logging.DEBUG):\n","    # Create a custom logger\n","    logger = logging.getLogger(name)\n","\n","    # Set the default logging level\n","    logger.setLevel(level)\n","\n","    # Create a handler for output to the console (stdout)\n","    console_handler = logging.StreamHandler(sys.stdout)\n","    console_handler.setLevel(level)\n","\n","    # Create a formatter and set it for the handler\n","    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    console_handler.setFormatter(formatter)\n","\n","    # Add the console handler to the logger\n","    logger.addHandler(console_handler)\n","\n","    # Optional: Log to a file if log_file is provided\n","    if log_file:\n","        file_handler = logging.FileHandler(log_file)\n","        file_handler.setLevel(level)\n","        file_handler.setFormatter(formatter)\n","        logger.addHandler(file_handler)\n","\n","    return logger"]},{"cell_type":"markdown","metadata":{"id":"ACYhh6EzsOYX"},"source":["**Create the DataLoader**\n","\n","Creates a DataLoader that reads the specific subject images (your training assets) and the regularization set used to preserve the model‚Äôs general visual knowledge while adapting to the new subject."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTSsZczRH8uJ"},"outputs":[],"source":["## Code Cell 5.11\n","\n","class DreamBoothDataset(Dataset):\n","    def __init__(\n","        self,\n","        instance_data_root,\n","        instance_prompt,\n","        tokenizer,\n","        class_data_root=None,\n","        class_prompt=None,\n","        size=512,\n","        center_crop=False,\n","    ):\n","        self.size = size\n","        self.center_crop = center_crop\n","        self.tokenizer = tokenizer\n","\n","        self.instance_data_root = Path(instance_data_root)\n","        if not self.instance_data_root.exists():\n","            raise ValueError(\"Instance images root doesn't exists.\")\n","\n","        self.instance_images_path = list(Path(instance_data_root).iterdir())\n","        self.num_instance_images = len(self.instance_images_path)\n","        self.instance_prompt = instance_prompt\n","        self._length = self.num_instance_images\n","\n","        if class_data_root is not None:\n","            self.class_data_root = Path(class_data_root)\n","            self.class_data_root.mkdir(parents=True, exist_ok=True)\n","            self.class_images_path = list(Path(class_data_root).iterdir())\n","            self.num_class_images = len(self.class_images_path)\n","            self._length = max(self.num_class_images, self.num_instance_images)\n","            self.class_prompt = class_prompt\n","        else:\n","            self.class_data_root = None\n","\n","        self.image_transforms = transforms.Compose(\n","            [\n","                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n","                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.5], [0.5]),\n","            ]\n","        )\n","\n","    def __len__(self):\n","        return self._length\n","\n","    def __getitem__(self, index):\n","        example = {}\n","        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n","        if not instance_image.mode == \"RGB\":\n","            instance_image = instance_image.convert(\"RGB\")\n","        example[\"instance_images\"] = self.image_transforms(instance_image)\n","        example[\"instance_prompt_ids\"] = self.tokenizer(\n","            self.instance_prompt,\n","            padding=\"do_not_pad\",\n","            truncation=True,\n","            max_length=self.tokenizer.model_max_length,\n","        ).input_ids\n","\n","        if self.class_data_root:\n","            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n","            if not class_image.mode == \"RGB\":\n","                class_image = class_image.convert(\"RGB\")\n","            example[\"class_images\"] = self.image_transforms(class_image)\n","            example[\"class_prompt_ids\"] = self.tokenizer(\n","                self.class_prompt,\n","                padding=\"do_not_pad\",\n","                truncation=True,\n","                max_length=self.tokenizer.model_max_length,\n","            ).input_ids\n","\n","        return example\n","\n","\n","class PromptDataset(Dataset):\n","    def __init__(self, prompt, num_samples):\n","        self.prompt = prompt\n","        self.num_samples = num_samples\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, index):\n","        example = {}\n","        example[\"prompt\"] = self.prompt\n","        example[\"index\"] = index\n","        return example"]},{"cell_type":"markdown","metadata":{"id":"qwKCcgOG31EQ"},"source":["**Collate Function for DataLoader**\n","\n","This function prepares each training batch by combining instance prompts and their corresponding images. It can also include additional class prompts and images to help the model maintain its general visual knowledge while learning the new subject."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m11y0EdqB8Oz"},"outputs":[],"source":["## Code Cell 5.12\n","\n","def collate_fn(examples, tokenizer, with_prior_preservation: bool=False):\n","    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n","    pixel_values = [example[\"instance_images\"] for example in examples]\n","\n","    # Combine class and instance examples for prior preservation\n","    if with_prior_preservation:\n","        input_ids += [example[\"class_prompt_ids\"] for example in examples]\n","        pixel_values += [example[\"class_images\"] for example in examples]\n","\n","    pixel_values = torch.stack(pixel_values)\n","    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n","\n","    input_ids = tokenizer.pad(\n","        {\"input_ids\": input_ids},\n","        padding=\"max_length\",\n","        return_tensors=\"pt\",\n","        max_length=tokenizer.model_max_length\n","    ).input_ids\n","\n","    batch = {\n","        \"input_ids\": input_ids,\n","        \"pixel_values\": pixel_values,\n","    }\n","    return batch"]},{"cell_type":"markdown","metadata":{"id":"D8Rr8NTY31EQ"},"source":["**Fine-Tuning Stable Diffusion Settings**\n","\n","These are the settings for **Fine-tuning Stable Diffusion (SD)**.\n","\n","- The **special token** (e.g., `sks`) is rarely used by the text encoder, so we update its embedding to \"memorize\" the patterns of your input images.\n","\n","- **with_prior_preservation = True** helps prevent overfitting, ensuring the fine-tuned SD model can still generate generic objects (e.g., a generic person) in addition to your specific subject (e.g., HT‚Äôs appearance).\n","\n","- **num_class_images = 20** specifies that 20 generic reference images (e.g., generic person) are used alongside your subject images during fine-tuning.\n","\n","- **max_train_steps = 400** sets the training to 400 steps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLPFyD5UIAVL"},"outputs":[],"source":["## Code Cell 5.13\n","\n","# The special token that is rarely used by the text encoder.\n","# We replace this token‚Äôs embedding to \"memorize\" the patterns of your personal images.\n","# For example, if you describe \"person\", Stable Diffusion will generate a generic person.\n","# But if you say \"sks person\", it will generate an image of your specific person pictures.\n","special_token = \"sks\"\n","\n","# The name of the object related to your specific-subject images.\n","object_name = \"person\"\n","\n","# The prompt to describe your specific subject images.\n","# A basic example: \"a photo of [special_token] [object_name]\".\n","instance_prompt = f\"a photo of {special_token} {object_name}\"\n","\n","# The directory where checkpoints and other outputs will be saved.\n","output_dir = \"logs\"\n","\n","# If `with_prior_preservation` is set to True, we create a regularization set to avoid overfitting.\n","with_prior_preservation = True\n","\n","# The weight assigned to the prior preservation loss.\n","prior_loss_weight = 0.5\n","\n","# Folder containing class images for prior preservation.\n","prior_preservation_class_folder = \"class_images\"\n","\n","# The prompt used to generate class images for regularization.\n","prior_preservation_class_prompt = f\"a photo of {object_name}\"\n","\n","# Minimum number of class images required for prior preservation .\n","num_class_images = 20\n","\n","# We set a lower number of training steps for this assignment.\n","max_train_steps = 400\n","\n","# Interval (in steps) at which checkpoints are saved.\n","checkpointing_steps = 200\n","\n","# Random seed for reproducibility.\n","seed = 42\n","\n","# Set batch size. If prior preservation is disabled, use batch size of 2; otherwise, set it to 1.\n","train_batch_size = 2 if not with_prior_preservation else 1\n","\n","# Learning rate for the training process.\n","learning_rate = 5e-06\n","\n","class Arguments:\n","    def __init__(self):\n","        self.pretrained_model_name_or_path = pretrained_model_name_or_path  # (str) Path to the pretrained model\n","        self.resolution = resolution  # (int) Input resolution for images (default: 512)\n","        self.instance_data_dir = subject_assets_path  # (str) Directory containing instance images\n","        self.instance_prompt = instance_prompt  # (str) Template for generating prompts related to instance images\n","        self.learning_rate = learning_rate  # (float) Learning rate for training\n","        self.max_train_steps = max_train_steps  # (int) Maximum number of training steps\n","        self.checkpointing_steps = checkpointing_steps  # (int) Interval (in steps) for saving checkpoints\n","        self.output_dir = output_dir  # (str) Directory to save checkpoints and output files\n","        self.train_batch_size = train_batch_size  # (int) Number of samples per batch for training\n","        self.seed = seed  # (int) Seed for random number generation to ensure reproducibility\n","\n","        self.mixed_precision = \"fp16\"  # (str) Use mixed precision (default: \"fp16\"); choose between \"fp16\" and \"bf16\"\n","        self.gradient_checkpointing = True  # (bool) Use gradient checkpointing to reduce memory usage (default: True)\n","        self.use_8bit_adam = True  # (bool) Use the 8-bit Adam optimizer for lower memory usage (default: True)\n","\n","        self.center_crop = True  # (bool) Whether to apply center cropping to images (default: True)\n","        self.train_text_encoder = False  # (bool) Whether to train the text encoder (default: False)\n","        self.gradient_accumulation_steps = 2  # (int) Steps to accumulate gradients before backpropagation (default: 2)\n","        self.max_grad_norm = 1.0  # (float) Maximum value for gradient clipping (default: 1.0)\n","        self.with_prior_preservation = with_prior_preservation  # (bool) Whether to apply prior preservation loss (default: False)\n","        self.prior_loss_weight = prior_loss_weight  # (float) Weight for prior preservation loss if enabled (default: 0.5)\n","        self.class_data_dir = prior_preservation_class_folder if with_prior_preservation else None  # (str or None) Directory for class images, used in prior preservation\n","        self.class_prompt = prior_preservation_class_prompt if with_prior_preservation else \"\"  # (str) Prompt for class images if using prior preservation\n","        self.num_class_images = num_class_images  # (int) Minimum number of class images needed for prior preservation\n","        self.lr_warmup_steps = 100  # (int) Number of warmup steps for the learning rate scheduler (default: 100)\n","        self.lr_scheduler = \"constant\"  # (str) Type of learning rate scheduler (default: \"constant\")\n","\n","# Create an instance of Arguments to use\n","args = Arguments()\n"]},{"cell_type":"markdown","metadata":{"id":"g0-GZkdCpaQt"},"source":["**Create Folder for Saving Checkpoints**\n","\n","Creates the directory specified in `args.output_dir` to save checkpoints. If the folder doesn't exist, it will be automatically created."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lp5zDZl9paQt"},"outputs":[],"source":["## Code Cell 5.14\n","Path(args.output_dir).mkdir(parents=True, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"cXoD1L0y31EQ"},"source":["**Create a Regularization Set for the Specified Class**\n","\n","The following code creates a regularization set for the given `object_name` and saves the images to `class_data_dir`.\n","A **regularization set** is a small set of generic class images used during fine-tuning to preserve the model‚Äôs general knowledge and reduce overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4GDl_G6B8O0"},"outputs":[],"source":["## Code Cell 5.15\n","\n","sample_batch_size = 2\n","if args.with_prior_preservation:\n","    class_images_dir = Path(args.class_data_dir)\n","    class_images_dir.mkdir(parents=True, exist_ok=True)\n","    cur_class_images = len(list(class_images_dir.iterdir()))\n","\n","    if cur_class_images < args.num_class_images:\n","        pipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch.float16, safety_checker=None).to(\"cuda\")\n","        pipeline.set_progress_bar_config(disable=True)\n","\n","        num_new_images = num_class_images - cur_class_images\n","        print(f\"Number of class images to sample: {num_new_images}.\")\n","\n","        sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n","        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=sample_batch_size)\n","\n","        for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n","            with torch.autocast(\"cuda\"):\n","                images = pipeline(example[\"prompt\"]).images\n","\n","            for i, image in enumerate(images):\n","                image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n","        pipeline = None\n","        gc.collect()\n","        del pipeline\n","        with torch.no_grad():\n","          torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"4uJUGzW-paQt"},"source":["**Print Parameter Count**\n","\n","This function displays the total and trainable number of parameters in the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGufXwP-paQt"},"outputs":[],"source":["## Code Cell 5.16\n","\n","def print_params(model):\n","    total = 0\n","    trainable = 0\n","    for name, p in model.named_parameters():\n","        n = p.numel()\n","        total += n\n","        if p.requires_grad:\n","            trainable += n\n","    pct = (trainable / total * 100) if total else 0.0\n","    def to_m(x):  # millions\n","        return f\"{x/1e6:.2f}M\"\n","\n","    print(\"-\" * 80)\n","    print(f\"Total: {to_m(total)} | Trainable: {to_m(trainable)} ({pct:.2f}%)\")"]},{"cell_type":"markdown","metadata":{"id":"MyZ3Buhl31ER"},"source":["**Function for Fine-Tuning Stable Diffusion**\n","\n","<font color='red'>**PART 5.1:**</font> [15 points]\n","\n","In this section, you will fine-tune a small subset of layers in the U-Net of Stable Diffusion v1.5.\n","\n","<font color='red'>**Deliverables**</font>\n","\n","- Unfreeze the Q, K, V projections and the `to_out` layer in all attention blocks of the U-Net.\n","- Set these parameters to require gradient updates during fine-tuning and append them to the `trainable_params` list.\n","- Keep all other U-Net parameters frozen.\n","- Also report the number of *tunable parameters* from your implementation (use the value printed by `print_params()`) in **Part 5.1** of your Overleaf project.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIJh5OgyILA6"},"outputs":[],"source":["## Code Cell 5.17\n","\n","def finetune_sd(collate_fn):\n","    logger = get_logger(__name__)\n","\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()  # Clear the GPU memory cache\n","\n","    # Load models and create wrapper for stable diffusion\n","    text_encoder = CLIPTextModel.from_pretrained(\n","        pretrained_model_name_or_path,\n","        subfolder=\"text_encoder\"\n","    )\n","    vae = AutoencoderKL.from_pretrained(\n","        pretrained_model_name_or_path,\n","        subfolder=\"vae\"\n","    )\n","    unet = UNet2DConditionModel.from_pretrained(\n","        pretrained_model_name_or_path,\n","        subfolder=\"unet\"\n","    )\n","    tokenizer = CLIPTokenizer.from_pretrained(\n","        pretrained_model_name_or_path,\n","        subfolder=\"tokenizer\",\n","    )\n","\n","    set_seed(args.seed)\n","\n","    accelerator = Accelerator(\n","        gradient_accumulation_steps=args.gradient_accumulation_steps,\n","        mixed_precision=args.mixed_precision,\n","    )\n","\n","    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n","        raise ValueError(\n","            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n","            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n","        )\n","\n","    vae.requires_grad_(False)\n","    if not args.train_text_encoder:\n","        text_encoder.requires_grad_(False)\n","\n","    if args.gradient_checkpointing:\n","        unet.enable_gradient_checkpointing()\n","        if args.train_text_encoder:\n","            text_encoder.gradient_checkpointing_enable()\n","\n","    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n","    if args.use_8bit_adam:\n","        optimizer_class = bnb.optim.AdamW8bit\n","    else:\n","        optimizer_class = torch.optim.AdamW\n","\n","    # ======================= IMPLEMENT HERE (START) =======================\n","    # Freeze everything in UNet\n","    for p in unet.parameters():\n","        p.requires_grad = False\n","\n","    # Unfreeze (enable training) for specific attention-related layers: Q, K, V, and to_out layers\n","    trainable_params = []\n","    for name, p in unet.named_parameters():\n","        # PART 5.1: Implement!\n","\n","    # ======================= IMPLEMENT HERE (END) =========================\n","\n","    print_params(unet)\n","\n","    optimizer = optimizer_class(\n","        trainable_params,\n","        lr=args.learning_rate,\n","    )\n","\n","    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n","\n","    train_dataset = DreamBoothDataset(\n","        instance_data_root=args.instance_data_dir,\n","        instance_prompt=args.instance_prompt,\n","        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n","        class_prompt=args.class_prompt,\n","        tokenizer=tokenizer,\n","        size=args.resolution,\n","        center_crop=args.center_crop,\n","    )\n","\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=args.train_batch_size,\n","        shuffle=True,\n","        collate_fn=lambda examples: collate_fn(examples, tokenizer, args.with_prior_preservation),\n","    )\n","\n","    lr_scheduler = get_scheduler(\n","        args.lr_scheduler,\n","        optimizer=optimizer,\n","        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n","        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n","    )\n","\n","    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n","        unet, optimizer, train_dataloader, lr_scheduler\n","    )\n","\n","    weight_dtype = torch.float32\n","    if accelerator.mixed_precision == \"fp16\":\n","        weight_dtype = torch.float16\n","    elif accelerator.mixed_precision == \"bf16\":\n","        weight_dtype = torch.bfloat16\n","\n","    vae.to(accelerator.device, dtype=weight_dtype)\n","    vae.decoder.to(\"cpu\")\n","\n","    if not args.train_text_encoder:\n","        text_encoder.to(accelerator.device, dtype=weight_dtype)\n","\n","    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n","\n","    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","\n","    logger.info(\"***** Running training *****\")\n","    logger.info(f\"  Num examples = {len(train_dataset)}\")\n","    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n","    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n","    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n","\n","    # Only show the progress bar once on each machine.\n","    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n","    progress_bar.set_description(\"Steps\")\n","    global_step = 0\n","\n","    for epoch in range(num_train_epochs):\n","        unet.train()\n","        for step, batch in enumerate(train_dataloader):\n","            with accelerator.accumulate(unet):\n","\n","                # 1. Transform images to latent space\n","                src_imgs = batch[\"pixel_values\"].to(dtype=weight_dtype) # Get images\n","                latents = vae.encode(src_imgs).latent_dist.sample() # Extract the latent distribution\n","                latents = latents * 0.18215 # Scale the latents\n","\n","                # 2. Generate noise to be added to the latents\n","                noise = torch.randn_like(latents) # Create random noise\n","                batch_size, channels, height, width = latents.shape\n","\n","                # 3. Select a random timestep for each image\n","                timesteps = torch.randint(\n","                    0, noise_scheduler.config.num_train_timesteps,\n","                    (batch_size,),\n","                    device=latents.device)\n","                timesteps = timesteps.long()\n","\n","                # 4. Add noise to the latents based on the noise level at each timestep\n","                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","                # 5. Retrieve the text embedding as a condition\n","                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n","\n","                # 6. Predict the noise residual\n","                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n","\n","                # Get the target for loss depending on the prediction type\n","                if noise_scheduler.config.prediction_type == \"epsilon\":\n","                    target = noise\n","                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n","                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n","                else:\n","                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n","\n","                if args.with_prior_preservation:\n","                    # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\n","                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n","                    target, target_prior = torch.chunk(target, 2, dim=0)\n","\n","                    # 7-1. Compute instance loss\n","                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n","\n","                    # 7-2. Compute prior loss\n","                    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n","\n","                    # 7-3. Add the prior loss to the instance loss\n","                    loss = loss + args.prior_loss_weight * prior_loss\n","                else:\n","                    # 7. Compute instance loss\n","                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n","\n","                accelerator.backward(loss)\n","\n","                if accelerator.sync_gradients:\n","                    accelerator.clip_grad_norm_(trainable_params, args.max_grad_norm)\n","\n","                optimizer.step()\n","                lr_scheduler.step()\n","                optimizer.zero_grad()\n","\n","            # Checks if the accelerator has performed an optimization step behind the scenes\n","            if accelerator.sync_gradients:\n","                progress_bar.update(1)\n","                global_step += 1\n","\n","                if global_step % args.checkpointing_steps == 0:\n","                    if accelerator.is_main_process:\n","\n","                        save_unet_path = Path(args.output_dir).joinpath(f\"unet_checkpoint-{global_step}.pth\")\n","                        torch.save(unet.state_dict(), save_unet_path)\n","\n","            # logs = {\"loss\": loss.detach().item()}\n","            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n","            progress_bar.set_postfix(**logs)\n","\n","            if global_step >= args.max_train_steps:\n","                break\n","\n","        accelerator.wait_for_everyone()\n","\n","    # Create the pipeline using using the trained modules and save it.\n","    if accelerator.is_main_process:\n","\n","        save_unet_path = Path(args.output_dir).joinpath(f\"unet_checkpoint-{global_step}.pth\")\n","        torch.save(unet.state_dict(), save_unet_path)\n"]},{"cell_type":"markdown","metadata":{"id":"DGJcV5Hk31ER"},"source":["**Start Fine-Tuning with `Accelerate`**\n","\n","This command starts running the `finetune_sd` function using `Accelerate`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8cbXiHYI56s"},"outputs":[],"source":["## Code Cell 5.18\n","\n","accelerate.notebook_launcher(finetune_sd, args=(collate_fn,))"]},{"cell_type":"markdown","metadata":{"id":"cIr9vMH5paQu"},"source":["**Clear Cache and Collect Garbage for Memory Management**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJxK3I8NsWva"},"outputs":[],"source":["## Code Cell 5.19\n","\n","# Before clearing cache and collecting garbage, run nvidia-smi to check current GPU memory usage.\n","!nvidia-smi\n","\n","# Clear GPU and CPU caches\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","# After clearing, run nvidia-smi again to confirm that unused memory has been released.\n","!nvidia-smi\n","print(\"‚úÖ Cache cleared and garbage collected.\")"]},{"cell_type":"markdown","metadata":{"id":"zBD1iBu1s4Zb"},"source":["**Load Pre-trained Stable Diffusion and Fine-tuned UNet Weights**\n","\n","This step loads the pre-trained Stable Diffusion model and the fine-tuned UNet weights for later inference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEcSg9YGNjyw"},"outputs":[],"source":["## Code Cell 5.20\n","\n","unet_checkpoint_name = \"unet_checkpoint-400.pth\"\n","save_unet_weight_path = os.path.join(args.output_dir, unet_checkpoint_name)\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    args.pretrained_model_name_or_path,\n","    torch_dtype=torch.float16,\n",").to(\"cuda\")\n","\n","unet_state_dict = torch.load(save_unet_weight_path)\n","\n","print(list(unet_state_dict.keys()))\n","msg = pipe.unet.load_state_dict(unet_state_dict, strict=False)\n","print(msg)\n","\n","pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n","\n","# Now the pipeline will use the DPMSolverMultistepScheduler\n","print(f\"Scheduler has been updated to: {type(pipe.scheduler)}\")\n"]},{"cell_type":"markdown","source":["\n","<font color='red'>**PART 5.2:**</font> [10 points]\n","\n","Generate images using the fine-tuned Stable Diffusion model.\n","\n","<font color='red'>**Deliverables**</font>\n","\n","From *Code Cell 5.23* to *Code Cell 5.30*, generate comparison images **with** the `sks` token and **without** it using the provided prompts.  \n","Then include those images in **Figure 1** of your Overleaf project by replacing each placeholder with the corresponding image you generated.\n"],"metadata":{"id":"ox5Mpr23SJZf"}},{"cell_type":"markdown","metadata":{"id":"gQQF_k6a31EU"},"source":["**Generate Unique Images of the Specific Subject (HT)**\n","\n","Let‚Äôs create new images that reflect HT‚Äôs appearance and visual patterns using the fine-tuned SD v1.5 model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hINQmcYIsggr"},"outputs":[],"source":["## Code Cell 5.23\n","\n","# (Example 1: with `sks` token) Define the text prompt for image generation using the special identifier token `sks`.\n","prompt = \"a photo of sks person next to the lake\"\n","negative_prompt = \"jpeg artifact, lowres\"\n","\n","num_inference_steps = 100\n","guidance_scale = 7.5\n","\n","num_images_per_prompt = 1\n","num_rows = 1\n","\n","seed = 0\n","generator = torch.Generator().manual_seed(seed)\n","\n","generated_images = pipe(\n","         prompt,\n","         negative_prompt=negative_prompt,\n","         num_images_per_prompt=num_images_per_prompt,\n","         num_inference_steps=num_inference_steps,\n","         guidance_scale=guidance_scale,\n","         generator=generator).images\n","\n","grid = image_grid(generated_images, rows=1, cols=num_images_per_prompt)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLm0BaUrpaQu"},"outputs":[],"source":["## Code Cell 5.24\n","\n","# (Example 1: without `sks` token) Define the text prompt for image generation using the special identifier token `sks`.\n","prompt = \"a photo of person next to the lake\"\n","negative_prompt = \"jpeg artifact, lowres\"\n","\n","num_inference_steps = 100\n","guidance_scale = 7.5\n","\n","num_images_per_prompt = 1\n","num_rows = 1\n","\n","seed = 0\n","generator = torch.Generator().manual_seed(seed)\n","\n","generated_images = pipe(\n","         prompt,\n","         negative_prompt=negative_prompt,\n","         num_images_per_prompt=num_images_per_prompt,\n","         num_inference_steps=num_inference_steps,\n","         guidance_scale=guidance_scale,\n","         generator=generator).images\n","\n","grid = image_grid(generated_images, rows=1, cols=num_images_per_prompt)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_8Tc2qYRpaQu"},"outputs":[],"source":["## Code Cell 5.25\n","\n","# (Example 2: with `sks` token) Define the text prompt for image generation using the special identifier token `sks`.\n","prompt = \"a photo of sks person in the office\"\n","negative_prompt = \"low quality, lowres, jpeg artifacts\"\n","\n","\n","num_inference_steps = 100\n","guidance_scale = 7.5\n","\n","num_images_per_prompt = 1\n","num_rows = 1\n","\n","seed = 0\n","generator = torch.Generator().manual_seed(seed)\n","\n","generated_images = pipe(\n","         prompt,\n","         negative_prompt=negative_prompt,\n","         num_images_per_prompt=num_images_per_prompt,\n","         num_inference_steps=num_inference_steps,\n","         guidance_scale=guidance_scale,\n","         generator=generator).images\n","\n","grid = image_grid(generated_images, rows=1, cols=num_images_per_prompt)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yxl-LVCVpaQu"},"outputs":[],"source":["## Code Cell 5.26\n","\n","# (Example 2: without `sks` token) Define the text prompt for image generation using the special identifier token `sks`.\n","prompt = \"a photo of person in the office\"\n","negative_prompt = \"low quality, lowres, jpeg artifacts\"\n","\n","\n","num_inference_steps = 100\n","guidance_scale = 7.5\n","\n","num_images_per_prompt = 1\n","num_rows = 1\n","\n","seed = 0\n","generator = torch.Generator().manual_seed(seed)\n","\n","generated_images = pipe(\n","         prompt,\n","         negative_prompt=negative_prompt,\n","         num_images_per_prompt=num_images_per_prompt,\n","         num_inference_steps=num_inference_steps,\n","         guidance_scale=guidance_scale,\n","         generator=generator).images\n","\n","grid = image_grid(generated_images, rows=1, cols=num_images_per_prompt)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lk90ItlpaQu"},"outputs":[],"source":["## Code Cell 5.27\n","\n","# (Example 3: with `sks` token) Define the text prompt for image generation using the special identifier token `sks`.\n","prompt = \"a photo of sks person at cafe\"\n","\n","\n","num_inference_steps = 100\n","guidance_scale = 7.5\n","\n","num_images_per_prompt = 1\n","num_rows = 1\n","\n","seed = 0\n","generator = torch.Generator().manual_seed(seed)\n","\n","generated_images = pipe(\n","         prompt,\n","         num_images_per_prompt=num_images_per_prompt,\n","         num_inference_steps=num_inference_steps,\n","         guidance_scale=guidance_scale,\n","         generator=generator).images\n","\n","grid = image_grid(generated_images, rows=1, cols=num_images_per_prompt)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EBqXVdSLpaQu"},"outputs":[],"source":["## Code Cell 5.28\n","\n","# (Example 3: without `sks` token) Define the text prompt for image generation using the special identifier token `sks`.\n","prompt = \"a photo of person at cafe\"\n","\n","\n","num_inference_steps = 100\n","guidance_scale = 7.5\n","\n","num_images_per_prompt = 1\n","num_rows = 1\n","\n","seed = 0\n","generator = torch.Generator().manual_seed(seed)\n","\n","generated_images = pipe(\n","         prompt,\n","         num_images_per_prompt=num_images_per_prompt,\n","         num_inference_steps=num_inference_steps,\n","         guidance_scale=guidance_scale,\n","         generator=generator).images\n","\n","grid = image_grid(generated_images, rows=1, cols=num_images_per_prompt)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96jTdP8NpaQu"},"outputs":[],"source":["## Code Cell 5.29\n","\n","# (Example 4: with `sks` token) Define the text prompt for image generation using the special identifier token `sks`.\n","prompt = \"a photo of sks person on the yard\"\n","\n","num_inference_steps = 100\n","guidance_scale = 7.5\n","\n","num_images_per_prompt = 1\n","num_rows = 1\n","\n","seed = 0\n","generator = torch.Generator().manual_seed(seed)\n","\n","generated_images = pipe(\n","         prompt,\n","         num_images_per_prompt=num_images_per_prompt,\n","         num_inference_steps=num_inference_steps,\n","         guidance_scale=guidance_scale,\n","         generator=generator).images\n","\n","grid = image_grid(generated_images, rows=1, cols=num_images_per_prompt)\n","grid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPv7HoONpaQv"},"outputs":[],"source":["## Code Cell 5.30\n","\n","# (Example 4: without `sks` token) Define the text prompt for image generation using the special identifier token `sks`.\n","prompt = \"a photo of person on the yard\"\n","\n","num_inference_steps = 100\n","guidance_scale = 7.5\n","\n","num_images_per_prompt = 1\n","num_rows = 1\n","\n","seed = 0\n","generator = torch.Generator().manual_seed(seed)\n","\n","generated_images = pipe(\n","         prompt,\n","         num_images_per_prompt=num_images_per_prompt,\n","         num_inference_steps=num_inference_steps,\n","         guidance_scale=guidance_scale,\n","         generator=generator).images\n","\n","grid = image_grid(generated_images, rows=1, cols=num_images_per_prompt)\n","grid"]},{"cell_type":"markdown","metadata":{"id":"gj8wm__fVuqe"},"source":["---\n","\n","### **6. Training-Free Multi-Prompt Generation with Varied Resolutions (Inference-Time)**\n","\n","In Stable Diffusion v1.5, one prompt drives the whole image and the default output is $512 \\times 512$. You could enable multi-prompt generation and varied resolutions (e.g., $512 \\times 1024$ or $1024 \\times 512$) by fine-tuning the model, but that adds training cost and must be repeated for each text-to-image model. In this part, you‚Äôll achieve both at inference time: we‚Äôll slice the latent into regions and guide each region with its own prompt during denoising, while also supporting varied output resolutions. This avoids any re-training, keeps compute to forward passes, and works directly with the pre-trained weights you already have.\n","\n","You will control different regions of the image with separate text prompts. For example:\n","\n","- üîµ The **left/upper partition** of the image is controlled by **<span style=\"color:#0070C0\">text prompt A</span>**.\n","- üü¢ The **right/lower partition** of the image is controlled by **<span style=\"color:#50C878\">text prompt B</span>**..\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1ZUoVGkQu5CAJkfWcnt2K7Z7bZHaSG8Nc\" width=\"1000\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"62JvzFwJVuqf"},"source":["**Installing the Necessary Libraries**\n","\n","Before running the code in this notebook, make sure you have all the required libraries installed. You can install them using the commands below:"]},{"cell_type":"markdown","source":["\n","> **‚öôÔ∏è Code Cell 6.1 ‚Äî Install Required Libraries**\n",">\n","> This cell installs all necessary libraries for the assignment (see **Code Cell: Setup & Prerequisites**).\n","\n","> - If you **have not** installed them yet, run **Code Cell: Setup & Prerequisites** first.\n","> - If you **already** ran it, you can **skip** this cell and proceed to the next one."],"metadata":{"id":"lkRdcwXQzl6n"}},{"cell_type":"markdown","metadata":{"id":"CGyB_q7lVuqg"},"source":["**Checking GPU Details**\n","\n","To make sure you're using a GPU and to check its details, you can run the following command:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpBnGljMVuqg"},"outputs":[],"source":["## Code Cell 6.2\n","\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"nhQzPXkIVuqg"},"source":["**Core Library Imports for Model Operations**\n","\n","Below are the libraries needed for this notebook, along with the specific versions used:\n","\n","- **PyTorch version:** 2.4.1+cu121\n","- **Diffusers version:** 0.30.3\n","- **Transformers version:** 4.44.2\n","- **xformers version:** 0.0.28\n","\n","\n","**Computing Resource**\n","\n","- GPU: 1√ó NVIDIA Tesla T4 (15,360 MB VRAM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0UqdOitZ4RL5"},"outputs":[],"source":["## Code Cell 6.3\n","\n","import torch\n","import diffusers\n","import transformers\n","import accelerate\n","import xformers\n","\n","# Expected versions\n","expected = {\n","    \"PyTorch\": \"2.4.1+cu121\",\n","    \"Diffusers\": \"0.30.3\",\n","    \"Transformers\": \"4.44.2\",\n","    \"Accelerate\": \"0.34.2\",\n","    \"Xformers\": \"0.0.28\",\n","}\n","\n","# Print detected versions\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Diffusers version: {diffusers.__version__}\")\n","print(f\"Transformers version: {transformers.__version__}\")\n","print(f\"Accelerate version: {accelerate.__version__}\")\n","print(f\"Xformers version: {xformers.__version__}\")\n","\n","# Check if they match\n","if (\n","    torch.__version__ == expected[\"PyTorch\"]\n","    and diffusers.__version__ == expected[\"Diffusers\"]\n","    and transformers.__version__ == expected[\"Transformers\"]\n","    and accelerate.__version__ == expected[\"Accelerate\"]\n","    and xformers.__version__ == expected[\"Xformers\"]\n","):\n","    print(\"Passed ‚úÖ\")\n","else:\n","    print(\"Version mismatch ‚ùå\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1NSUYfA4RL5"},"outputs":[],"source":["## Code Cell 6.4\n","\n","import PIL\n","from PIL import Image\n","\n","from tqdm import tqdm\n","\n","from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n","\n","import gc\n","import random\n","import numpy as np\n","from einops import rearrange\n","from typing import List\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["\n","**Troubleshooting for Code Cell 6.4**\n","\n","> <font color='green'>‚ö†Ô∏è NOTE ‚Äî Only if `AutoencoderKL` import fails when running Code Cell 6.4</font>\n","\n",">\n","> If you cannot import `AutoencoderKL` from `diffusers`, follow the steps below.  \n",">\n","> If the import works, skip this section and go directly to *Code Cell 6.5*.\n","\n","If you encounter this error:\n","\n","```bash\n","RuntimeError: Failed to import diffusers.models.autoencoders.autoencoder_kl because of the following error (look up to see its traceback):\n","Failed to import transformers.models.auto.image_processing_auto because of the following error (look up to see its traceback):\n","partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)\n","```\n","\n","- **Step 1**. Create a new *code cell*  by clicking `+ Code` from the toolbar at the top and paste the following line to Uninstall conflicting packages:\n","```bash\n","!pip uninstall -y diffusers torch torchvision torchaudio xformers accelerate\n","```\n","\n","- **Step 2.** Reinstall by running Code Cell: Setup & Prerequisites again.\n","\n","- **Step 3.** After reinstall completes, run *Code Cell 6.4* to import the packages. If imports succeed, proceed to *Code Cell 6.5*."],"metadata":{"id":"qPLqdoOXdewG"}},{"cell_type":"markdown","metadata":{"id":"o6K-GY3xVuqg"},"source":["**Model Selection: Stable Diffusion v1.5**\n","\n","We have selected **Stable Diffusion v1.5** for its balanced performance in image quality and resource efficiency.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4l1x5Q-4RL6"},"outputs":[],"source":["## Code Cell 6.5\n","\n","pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\""]},{"cell_type":"markdown","metadata":{"id":"EfA4t-bRVuqh"},"source":["**Memory Efficient Attention using Xformers**\n","\n","This function implements memory-efficient attention using Xformers for optimized performance with large models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEirEH23Vuqh"},"outputs":[],"source":["## Code Cell 6.6\n","\n","def _memory_efficient_attention_xformers(\n","        module: torch.nn.Module,\n","        query: torch.Tensor,\n","        key: torch.Tensor,\n","        value: torch.Tensor) -> torch.Tensor:\n","    query = query.contiguous()\n","    key = key.contiguous()\n","    value = value.contiguous()\n","    hidden_states = xformers.ops.memory_efficient_attention(query, key, value,attn_bias=None)\n","    hidden_states = module.batch_to_head_dim(hidden_states)\n","    return hidden_states"]},{"cell_type":"markdown","metadata":{"id":"xl6M3DtqVuqh"},"source":["**Text Embedding Generation from Prompts**\n","\n","This function encodes a list of text prompts into embeddings using a tokenizer and text encoder.  \n","It uses float16 for more efficient processing on the GPU.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNztYdkdVuqh"},"outputs":[],"source":["## Code Cell 6.7\n","\n","def encode_prompts(tokenizer,\n","                   text_encoder,\n","                   prompts: list,\n","                   dtype: torch.dtype=torch.float16,\n","                   device: str=\"cuda\") -> torch.Tensor:\n","\n","        '''\n","        Generate text embeddings from a list of prompts using the tokenizer and text encoder.\n","        '''\n","        with torch.no_grad():\n","            tokens = tokenizer(prompts,\n","                               max_length=tokenizer.model_max_length,\n","                               padding=True,\n","                               truncation=True,\n","                               return_tensors='pt').input_ids.to(device)\n","            text_embeddings = text_encoder(tokens,\n","                                           output_hidden_states=True).last_hidden_state.to(device, dtype=dtype)\n","        return text_embeddings"]},{"cell_type":"markdown","metadata":{"id":"zxKFcgS9Vuqh"},"source":["**Decoding Latents to Images**\n","\n","This function decodes latent representations into a list of PIL images using the VAE (Variational Autoencoder). Below is a breakdown of the key steps:\n","\n","1. `latents = 1 / 0.18215 * latents`: This rescales the latent values by a factor to match the expected input for the VAE decoder.\n","2. `images = vae.decode(latents).sample`: The VAE decoder processes the latents and returns the decoded image tensors.\n","3. `images = (images / 2 + 0.5).clamp(0, 1)`: This normalizes the pixel values to fall between 0 and 1.\n","4. `images = images.cpu().permute(0, 2, 3, 1)`: Moves the channels dimension to the last position to match the standard image format (height, width, channels).\n","5. `images = (images * 255).round().astype(\"uint8\")`: Scales the pixel values to the 0-255 range and converts them into the uint8 format.\n","6. `Image.fromarray(image)`: Converts the numpy array into a PIL image format for easy visualization or saving.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJDzH9dBVuqh"},"outputs":[],"source":["## Code Cell 6.8\n","\n","def decode_latents(vae, latents: torch.Tensor) -> List[PIL.Image.Image]:\n","    latents = 1 / 0.18215 * latents\n","    with torch.no_grad():\n","        images = vae.decode(latents).sample\n","    images = (images / 2 + 0.5).clamp(0, 1)\n","    images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n","    images = (images * 255).round().astype(\"uint8\")\n","    pil_images = [Image.fromarray(image) for image in images]\n","    return pil_images"]},{"cell_type":"markdown","metadata":{"id":"xUdN78VjVuqh"},"source":["**Horizontal Partitioned Image Controller**\n","\n","<font color='red'>**PART 6.1:**</font> [10 points]\n","\n","In this section, you will work on matching a text prompt to specific partitions of an image by focusing on the cross-attention mechanism.\n","\n","<font color='red'>**Deliverables**</font>\n","\n","1. **Using Hook Method for Cross Attention in UNet:**\n","   - Defined in `hook_forward(self.unet)`.\n","   - We use the hook method to modify the cross attention layers in the UNet model without directly changing the original source code.\n","   - Hooks allow us to insert custom functions at specific points in the model (such as during the forward pass), which gives us flexibility and control over the behavior of layers during training or inference.\n","\n","2. **Loading the Pre-trained Stable Diffusion (SD) Model:**\n","   - Defined in `initialized_SD`.\n","   - Load the pre-trained Stable Diffusion model and split it into the following components:\n","     - **Tokenizer**: Converts the text prompts into tokens.\n","     - **Text Encoder**: Encodes the tokens into embeddings.\n","     - **VAE (Variational Autoencoder)**: Manages image encoding and decoding.\n","     - **UNet**: Processes the latent representations to generate the final image.\n","\n","3. **Defining a New Operation for Cross Attention:**\n","   - Define a function called `cross_attention_hook_forward` to modify the behavior of the cross-attention layers.\n","   - *You will need to match the prompt describing the left partition to the corresponding partition in the image features, and the same for the right partition*.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OjV7EsyVuqh"},"outputs":[],"source":["## Code Cell 6.9\n","\n","class HorizontalPartitionedImageController(object):\n","    def __init__(self,\n","                 pretrained_model_name_or_path: str,\n","                 dtype: torch.dtype=torch.float16,\n","                 device: str=\"cuda\"):\n","\n","        self.initialize_SD(pretrained_model_name_or_path, dtype, device)\n","\n","        self.dtype = dtype\n","        self.device = device\n","\n","        self.hook_forwards(self.unet)\n","\n","    def initialize_SD(self,\n","                      pretrained_model_name_or_path: str,\n","                      dtype: torch.dtype=torch.float16,\n","                      device: str=\"cuda\"):\n","\n","        gc.collect()\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()  # Clear the GPU memory cache for every run\n","\n","        self.tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder='tokenizer')\n","        self.text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder='text_encoder').eval().to(device, dtype=dtype)\n","\n","        self.vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder='vae').eval().to(device, dtype=dtype)\n","        self.vae.enable_slicing()\n","\n","        self.unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder='unet').eval().to(device, dtype=dtype)\n","        self.unet.set_use_memory_efficient_attention_xformers(True)\n","\n","        self.scheduler = DDIMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n","\n","    def __call__(\n","                self,\n","                prompts: list,\n","                negative_prompt: str=\"\",\n","                img_height: int=512,\n","                img_width: int=1024,\n","                batch_size: int=2,\n","                guidance_scale: float=7.5,\n","                num_inference_steps: int=50,\n","                base_ratio: float=0.3,\n","                seed: int=0,\n","            ):\n","\n","        self.set_seed(seed=seed)\n","        self.latent_height, self.latent_width = img_height // 8, img_width // 8\n","        self.latent_pixels = self.latent_height * self.latent_width\n","        self.base_ratio = base_ratio\n","\n","        '''\n","        prompts (list): includes both the base prompt and partition-specific prompts\n","        '''\n","\n","        # Repeat each prompt (base, partition-specific, and negative) by the batch size\n","        # Format: [\n","        #     the_positive_prompt_for_base_view,\n","        #     the_positive_prompt_for_left_parition,\n","        #     the_positive_prompt_for_right_parition,\n","        #     the_negative_prompt\n","        # ]\n","        all_prompts = []\n","        for prompt in prompts:\n","            all_prompts.extend([prompt] * batch_size)\n","        all_prompts.extend([negative_prompt] * batch_size)\n","\n","        # Generate embeddings for the text prompts (base, partition-specific, and negative)\n","        text_embeddings: torch.Tensor = encode_prompts(\n","            tokenizer=self.tokenizer,\n","            text_encoder=self.text_encoder,\n","            prompts=all_prompts,\n","            dtype=self.dtype,\n","            device=self.device)\n","\n","        # Set the timesteps for the scheduler based on the number of inference steps\n","        self.scheduler.set_timesteps(num_inference_steps, device=self.device)\n","        timesteps = self.scheduler.timesteps # (shape) num_inference_steps, (dtype) torch.float16\n","\n","        latents = torch.randn(batch_size, 4, self.latent_height, self.latent_width).to(self.device, dtype = self.dtype)\n","        latents = latents * self.scheduler.init_noise_sigma # (dtype) torch.float16\n","\n","        progress_bar = tqdm(range(num_inference_steps),\n","                            desc=\"Generating Image ...\",\n","                            leave=False)\n","\n","        for _, t in enumerate(timesteps):\n","            latent_model_input = torch.cat([latents] * 2) # Duplicate latents for both positive and negative prompts\n","            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n","\n","            # Predict the noise for the current timestep using the UNet model\n","            with torch.no_grad():\n","                noise_pred = self.unet(\n","                    sample=latent_model_input,\n","                    timestep=t,\n","                    encoder_hidden_states=text_embeddings).sample\n","\n","            # Apply negative Classifier-Free Guidance\n","            (\n","                noise_pred_text,\n","                noise_pred_negative\n","            ) = noise_pred.chunk(2)\n","            noise_pred = noise_pred_negative + guidance_scale * (noise_pred_text - noise_pred_negative)\n","\n","            # Obtain the denoised latents for the current timestep\n","            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n","\n","            progress_bar.update(1)\n","\n","        images: List[PIL.Image.Image] = decode_latents(\n","                        vae=self.vae,\n","                        latents=latents)\n","\n","        return images\n","\n","    def cross_attention_hook_forward(self, module):\n","        def forward(hidden_states: torch.Tensor,\n","                    encoder_hidden_states: torch.Tensor=None,\n","                    attention_mask=None):\n","\n","            visual_features, textual_features = hidden_states, encoder_hidden_states\n","\n","            # Shape: (batch_size * 2) x num_seqs x (num_heads * num_channels),\n","            # where the 2 represents the positive and negative prompts\n","            query = module.to_q(visual_features)\n","\n","            context = textual_features if textual_features is not None else visual_features\n","            key = module.to_k(context)\n","            value = module.to_v(context)\n","\n","            # Duplicate query for left/right attention:\n","            # First three queries are attended by the positive prompt (base, left, right),\n","            # and the fourth is attended by the negative prompt.\n","            query_attened_by_pos_prompt, query_attened_by_neg_prompt = query.chunk(2)\n","            query = torch.cat([\n","                query_attened_by_pos_prompt, # base view\n","                query_attened_by_pos_prompt, # for left controlling\n","                query_attened_by_pos_prompt, # for right controlling\n","                query_attened_by_neg_prompt,\n","            ], dim=0)\n","\n","            \"\"\"\n","            The `textual_features` tensor should follow this order:\n","\n","            1. Positive prompt: base view\n","            2. Positive prompt: text prompt for the left partition\n","            3. Positive prompt: text prompt for the right partition\n","            4. Negative prompt\n","\n","            These prompts are stacked along the `batch` dimension.\n","\n","            Following this order, the attention mechanism will perform independent operations as follows:\n","\n","            - Base view query will attend to the base view key and value:\n","                query_base_view <- key_base_view, value_base_view\n","\n","            - Left partition query will attend to the left partition key and value:\n","                query_left_partition <- key_left_partition, value_left_partition\n","\n","            - Right partition query will attend to the right partition key and value:\n","                query_right_partition <- key_right_partition, value_right_partition\n","\n","            - Negative prompt query will attend to the negative prompt key and value:\n","                query_neg_prompt <- key_neg_prompt, value_neg_prompt\n","            \"\"\"\n","\n","            # Reshape the query, key, and value tensors:\n","            # Move the head dimension into the batch dimension for efficient attention computation\n","            # New shape: (batch_size * 2 * num_heads) x num_seqs x channels\n","            query = module.head_to_batch_dim(query)\n","            key = module.head_to_batch_dim(key)\n","            value = module.head_to_batch_dim(value)\n","\n","            # Perform attention using memory-efficient xformers\n","            hidden_states = _memory_efficient_attention_xformers(module, query, key, value)\n","\n","            # xformers may return output in fp32, so we convert it back to the original data type (matching the query tensor)\n","            hidden_states = hidden_states.to(query.dtype)\n","\n","            # Calculate the downsample rate based on the latent pixel count and the query shape\n","            downsample_rate = int((self.latent_pixels // query.shape[1]) ** 0.5)\n","\n","            height = self.latent_height // downsample_rate\n","            width = self.latent_width // downsample_rate\n","\n","            # PART 6.1 (a): Implement!\n","            # Split the hidden states into four parts: base, left, right, and negative prompt\n","            (\n","                hidden_states_base,\n","                hidden_states_left,\n","                hidden_states_right,\n","                hidden_states_for_neg_prompt\n","            ) = # TODO: IMPLEMENT!\n","\n","            # PART 6.1 (b): Implement!\n","            # Reshape the left hidden states into batch_size x height x width x channels_with_heads\n","            # height and width are provided, please specify batch_size and channels_with_heads dimensions.\n","            hidden_states_left = # TODO: IMPLEMENT!\n","\n","            # PART 6.1 (c): Implement!\n","            # Reshape the right hidden states into batch_size x height x width x channels_with_heads\n","            # height and width are provided, please specify batch_size and channels_with_heads dimensions.\n","            hidden_states_right = # TODO: IMPLEMENT!\n","\n","            # PART 6.1 (d): Implement!\n","            # Extract the left half of the width for the left hidden states\n","            hidden_states_left = hidden_states_left[\n","                # TODO: IMPLEMENT!\n","            ]\n","\n","            # PART 6.1 (e): Implement!\n","            # Extract the right half of the width for the right hidden states\n","            hidden_states_right = hidden_states_right[\n","                # TODO: IMPLEMENT!\n","            ]\n","\n","            # PART 6.1 (f) and (g): Implement!\n","            # Concatenate the left and right hidden states along the width dimension and reshape the resulting tensor.\n","            # [Implement!] PART 6.1 (f) Concatenate the left and right hidden states along the width dimension\n","            # [Implement!] PART 6.1 (g) Reshape the concatenated tensor back into batch_size x (height * width) x channels\n","            hidden_states_controlled = # TODO: IMPLEMENT!\n","\n","            hidden_states_for_pos_prompt: torch.Tensor = (\n","                hidden_states_controlled * (1. - self.base_ratio) +\n","                hidden_states_base * self.base_ratio\n","            )\n","\n","            hidden_states = torch.cat([\n","                hidden_states_for_pos_prompt,\n","                hidden_states_for_neg_prompt\n","            ])\n","\n","            # Linear projection layer\n","            hidden_states = module.to_out[0](hidden_states)\n","            # Dropout layer\n","            hidden_states = module.to_out[1](hidden_states)\n","\n","            return hidden_states\n","\n","        return forward\n","\n","    # Overwrite the forward() method for UNet's cross-attention layers\n","    def hook_forwards(self, root_module: torch.nn.Module):\n","        for name, module in root_module.named_modules():\n","            # PART 6.1 (h): Implement!\n","            # Please specify the CrossAttention Layer to be hooked.\n","            # (Hint) It is better to check the model structure of self.unet for identifying the appropriate layers.\n","\n","            if :#TODO: IMPLEMENT!\n","                module.forward = #TODO: IMPLEMENT!\n","\n","    # set random seed\n","    def set_seed(self, seed=0):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.use_deterministic_algorithms = True"]},{"cell_type":"markdown","metadata":{"id":"J0NtEkqDVuqi"},"source":["**Initializing the Horizontal Partitioned Image Controller**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiwjsEGbVuqi"},"outputs":[],"source":["## Code Cell 6.10\n","\n","horizontal_controller = HorizontalPartitionedImageController(\n","                pretrained_model_name_or_path,\n","                dtype = torch.float16)"]},{"cell_type":"markdown","metadata":{"id":"mVp30JqWVuqi"},"source":["**Generating Images with Horizontal Partitioned Prompts**\n","\n","In this example, you will generate an image with specific prompts for different partitions of the image.\n","\n","**Positive Prompt**\n","\n","The `prompt_layout` is defined as:\n","\n","```python\n","prompt_layout = [\n","    \"<base-prompt>\",               # The base prompt provides a general or global view for the entire image. This prompt influences the overall tone or theme of the image.\n","    \"<the-prompt-for-left-partition>\",   # The prompt for the left partition specifically influences the left side of the image.\n","    \"<the-prompt-for-right-partition>\",  # The prompt for the right partition specifically influences the right side of the image.\n","]\n","```\n","\n","- **Base Prompt**:  \n","  `<base-prompt>`  \n","  The base prompt provides a general or global view for the entire image. This prompt influences the overall tone or theme of the image.\n","\n","- **Left Partition Prompt**:  \n","  `<the-prompt-for-left-partition>`  \n","  This prompt specifically influences the left side of the image.\n","\n","- **Right Partition Prompt**:  \n","  `<the-prompt-for-right-partition>`  \n","  This prompt specifically influences the right side of the image.\n","\n","**Negative Prompt**\n","\n","The `negative_prompt` helps guide the model away from unwanted features:\n","\n","- **Negative Prompt**:  \n","  `\"worst quality, low quality, bad anatomy, jpeg artifacts\"`  \n","  This discourages the model from generating low-quality features or undesirable artifacts, ensuring a higher-quality output.\n","\n","**Image Dimensions**\n","\n","To create a wider image, we use the following dimensions:\n","\n","- **Image Height**: 512  \n","- **Image Width**: 1024  \n","\n","This configuration produces an image that is wider than it is tall.\n","\n","**Base Ratio**\n","\n","The `base_ratio` controls the influence of the global/base prompt versus the partition-specific prompts:\n","\n","- **Base Ratio**: 0.1  \n","  A lower `base_ratio` (in this case, 0.1) means that the image generation will be mostly influenced by the prompts for the partitions (left and right), while the global base prompt has a smaller impact.\n","\n","**Guidance Scale**\n","\n","The `guidance_scale` controls how strongly the model should follow the prompts.\n"]},{"cell_type":"markdown","metadata":{"id":"UktlidcdVuqi"},"source":["**Drawing the Image with Partitioned Prompts**\n","\n","Now we are going to generate an image where:\n","\n","- The **left partition** is controlled by the prompt:  \n","  \"a dog wearing a hat and jacket\"\n","\n","- The **right partition** is controlled by the prompt:  \n","  \"cafe at Times Square, snowy\"\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1nLmPZrBYeW2jZLXccWQTPIiEb-Ys_ejz\" width=\"500\"/>\n"]},{"cell_type":"markdown","source":["<font color='red'>**PART 6.2:**</font> [5 points]\n","\n","Generate images using horizontal partitioned prompts.\n","\n","<font color='red'>**Deliverables**</font>\n","\n","From *Code Cells 6.11‚Äì6.13*, generate the images and include them in **Part 6.2** of your Overleaf project."],"metadata":{"id":"c5kq9-d_XpUu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEt-u0B6Vuqi"},"outputs":[],"source":["## Code Cell 6.11\n","\n","prompt_layout = [\n","    \"4k, high quality\",\n","    \"a dog wearing a hat and jacket\",\n","    \"cafe at Times Square, snowy\",\n","]\n","negative_prompt = \"worst quality, low quality, bad anatomy, jpeg artifacts\"\n","generated_images = horizontal_controller(\n","            prompt_layout,\n","            negative_prompt,\n","            img_height = 512,\n","            img_width = 1024,\n","            batch_size = 1, # batch size\n","            num_inference_steps=100, # inference sampling step\n","            base_ratio=0.1, # the weight assigned to the base prompt. A value of 0 means all prompts are partition-specific, while a value of 1 means all prompts are base.\n","            guidance_scale=7.5,\n","            seed=0,\n",")\n","\n","plt.imshow(generated_images[0])\n","plt.axis('off')  # Hide axes\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SI6rLoDjVuqi"},"source":["**Drawing the Image with Partitioned Prompts**\n","\n","Now we are going to generate an image where:\n","\n","- The **left partition** is controlled by the prompt:  \n","  \"cows on the farm\"\n","\n","- The **right partition** is controlled by the prompt:  \n","  \"tractor in front of the barn\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2geP9CsVuqi"},"outputs":[],"source":["## Code Cell 6.12\n","\n","prompt_layout = [\n","    \"4k, high quality\",\n","    \"cows on the farm\",\n","    \"tractor in front of the barn\",\n","]\n","negative_prompt = \"worst quality, low quality, bad anatomy, jpeg artifacts\"\n","generated_images = horizontal_controller(\n","            prompt_layout,\n","            negative_prompt,\n","            img_height = 512,\n","            img_width = 1024,\n","            batch_size = 1, # batch size\n","            num_inference_steps=100, # inference sampling step\n","            base_ratio=0.1, # the weight assigned to the base prompt. A value of 0 means all prompts are partition-specific, while a value of 1 means all prompts are base.\n","            guidance_scale=7.5,\n","            seed=0,\n",")\n","\n","plt.imshow(generated_images[0])\n","plt.axis('off')  # Hide axes\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YHvB29WBVuqj"},"source":["**Drawing the Image with Partitioned Prompts**\n","\n","Now we are going to generate an image where:\n","\n","- The **left partition** is controlled by the prompt:  \n","  \"bench under a tree, foliage, fall\"\n","\n","- The **right partition** is controlled by the prompt:  \n","  \"a bird flying above the lake\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB2naOPEVuqj"},"outputs":[],"source":["## Code Cell 6.13\n","\n","prompt_layout = [\n","    \"4k, high quality\",\n","    \"bench under a tree, foliage, fall\",\n","    \"a bird flying above the lake\",\n","]\n","negative_prompt = \"worst quality, low quality, bad anatomy, jpeg artifacts\"\n","generated_images = horizontal_controller(\n","            prompt_layout,\n","            negative_prompt,\n","            img_height = 512,\n","            img_width = 1024,\n","            batch_size = 1, # batch size\n","            num_inference_steps=100, # inference sampling step\n","            base_ratio=0.1, # the weight assigned to the base prompt. A value of 0 means all prompts are partition-specific, while a value of 1 means all prompts are base.\n","            guidance_scale=7.5,\n","            seed=0,\n",")\n","\n","plt.imshow(generated_images[0])\n","plt.axis('off')  # Hide axes\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DWBnbL3BVuqj"},"source":["**Vertical Partitioned Image Controller**\n","\n","<font color='red'>**PART 6.3:**</font> [10 points]\n","\n","In this section, you will work on matching a text prompt to specific vertical partitions of an image by focusing on the cross-attention mechanism.\n","\n","<font color='red'>**Deliverables**</font>\n","\n","1. **Using Hook Method for Cross Attention in UNet:**\n","   - Defined in `hook_forward(self.unet)`.\n","   - We use the hook method to modify the cross attention layers in the UNet model without directly changing the original source code.\n","   - Hooks allow us to insert custom functions at specific points in the model (such as during the forward pass), which gives us flexibility and control over the behavior of layers during training or inference.\n","\n","2. **Loading the Pre-trained Stable Diffusion (SD) Model:**\n","   - Defined in `initialized_SD`.\n","   - Load the pre-trained Stable Diffusion model and split it into the following components:\n","     - **Tokenizer**: Converts the text prompts into tokens.\n","     - **Text Encoder**: Encodes the tokens into embeddings.\n","     - **VAE (Variational Autoencoder)**: Manages image encoding and decoding.\n","     - **UNet**: Processes the latent representations to generate the final image.\n","\n","3. **Defining a New Operation for Cross Attention:**\n","   - Define a function called `cross_attention_hook_forward` to modify the behavior of the cross-attention layers.\n","   - *You will need to match the prompt describing the upper partition to the corresponding partition in the image features, and the same for the lower partition*.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZFZuTAiVuqj"},"outputs":[],"source":["## Code Cell 6.14\n","\n","class VerticalPartitionedImageController(object):\n","    def __init__(self,\n","                 pretrained_model_name_or_path: str,\n","                 dtype: torch.dtype=torch.float16,\n","                 device: str=\"cuda\"):\n","\n","        self.initialize_SD(pretrained_model_name_or_path, dtype, device)\n","\n","        self.dtype = dtype\n","        self.device = device\n","\n","        self.hook_forwards(self.unet)\n","\n","    def initialize_SD(self,\n","                      pretrained_model_name_or_path: str,\n","                      dtype: torch.dtype=torch.float16,\n","                      device: str=\"cuda\"):\n","\n","        gc.collect()\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()  # Clear the GPU memory cache\n","\n","        self.tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_name_or_path, subfolder='tokenizer')\n","        self.text_encoder = CLIPTextModel.from_pretrained(pretrained_model_name_or_path, subfolder='text_encoder').eval().to(device, dtype=dtype)\n","\n","        self.vae = AutoencoderKL.from_pretrained(pretrained_model_name_or_path, subfolder='vae').eval().to(device, dtype=dtype)\n","        self.vae.enable_slicing()\n","\n","        self.unet = UNet2DConditionModel.from_pretrained(pretrained_model_name_or_path, subfolder='unet').eval().to(device, dtype=dtype)\n","        self.unet.set_use_memory_efficient_attention_xformers(True)\n","\n","        self.scheduler = DDIMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n","\n","    def __call__(\n","                self,\n","                prompts: list,\n","                negative_prompt: str=\"\",\n","                img_height: int=512,\n","                img_width: int=1024,\n","                batch_size: int=2,\n","                guidance_scale: float=7.5,\n","                num_inference_steps: int=50,\n","                base_ratio: float=0.3,\n","                seed: int=0,\n","            ):\n","\n","        self.set_seed(seed=seed)\n","        self.latent_height, self.latent_width = img_height // 8, img_width // 8\n","        self.latent_pixels = self.latent_height * self.latent_width\n","        self.base_ratio = base_ratio\n","\n","        '''\n","        prompts (list): includes both the base prompt and partition-specific prompts\n","        '''\n","\n","        # Repeat each prompt (base, partition-specific, and negative) by the batch size\n","        # Format: [\n","        #     the_positive_prompt_for_base_view,\n","        #     the_positive_prompt_for_upper_parition,\n","        #     the_positive_prompt_for_lower_parition,\n","        #     the_negative_prompt\n","        # ]\n","        all_prompts = []\n","        for prompt in prompts:\n","            all_prompts.extend([prompt] * batch_size)\n","        all_prompts.extend([negative_prompt] * batch_size)\n","\n","        # Generate embeddings for the text prompts (base, partition-specific, and negative)\n","        text_embeddings: torch.Tensor = encode_prompts(\n","            tokenizer=self.tokenizer,\n","            text_encoder=self.text_encoder,\n","            prompts=all_prompts,\n","            dtype=self.dtype,\n","            device=self.device)\n","\n","        # Set the timesteps for the scheduler based on the number of inference steps\n","        self.scheduler.set_timesteps(num_inference_steps, device=self.device)\n","        timesteps = self.scheduler.timesteps # (shape) num_inference_steps, (dtype) torch.float16\n","\n","        latents = torch.randn(batch_size, 4, self.latent_height, self.latent_width).to(self.device, dtype = self.dtype)\n","        latents = latents * self.scheduler.init_noise_sigma # (dtype) torch.float16\n","\n","        progress_bar = tqdm(range(num_inference_steps),\n","                            desc=\"Generating Image ...\",\n","                            leave=False)\n","\n","        for _, t in enumerate(timesteps):\n","            latent_model_input = torch.cat([latents] * 2) # Duplicate latents for both positive and negative prompts\n","            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n","\n","            # Predict the noise for the current timestep using the UNet model\n","            with torch.no_grad():\n","                noise_pred = self.unet(\n","                    sample=latent_model_input,\n","                    timestep=t,\n","                    encoder_hidden_states=text_embeddings).sample\n","\n","            # Apply negative Classifier-Free Guidance\n","            (\n","                noise_pred_text,\n","                noise_pred_negative\n","            ) = noise_pred.chunk(2)\n","            noise_pred = noise_pred_negative + guidance_scale * (noise_pred_text - noise_pred_negative)\n","\n","            # Obtain the denoised latents for the current timestep\n","            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n","\n","            progress_bar.update(1)\n","\n","        images: List[PIL.Image.Image] = decode_latents(\n","                        vae=self.vae,\n","                        latents=latents)\n","\n","        return images\n","\n","    def cross_attention_hook_forward(self, module):\n","        def forward(hidden_states: torch.Tensor,\n","                    encoder_hidden_states: torch.Tensor=None,\n","                    attention_mask=None):\n","\n","            visual_features, textual_features = hidden_states, encoder_hidden_states\n","\n","            # Shape: (batch_size * 2) x num_seqs x (num_heads * num_channels),\n","            # where the 2 represents the positive and negative prompts\n","            query = module.to_q(visual_features)\n","\n","            context = textual_features if textual_features is not None else visual_features\n","            key = module.to_k(context)\n","            value = module.to_v(context)\n","\n","            # Duplicate query for upper/lower attention:\n","            # First three queries are attended by the positive prompt (base, upper, lower),\n","            # and the fourth is attended by the negative prompt.\n","            query_attened_by_pos_prompt, query_attened_by_neg_prompt = query.chunk(2)\n","            query = torch.cat([\n","                query_attened_by_pos_prompt, # base view\n","                query_attened_by_pos_prompt, # for upper controlling\n","                query_attened_by_pos_prompt, # for lower controlling\n","                query_attened_by_neg_prompt,\n","            ], dim=0)\n","\n","            \"\"\"\n","            The `textual_features` tensor should follow this order:\n","\n","            1. Positive prompt: base view\n","            2. Positive prompt: text prompt for the upper partition\n","            3. Positive prompt: text prompt for the lower partition\n","            4. Negative prompt\n","\n","            These prompts are stacked along the `batch` dimension.\n","\n","            Following this order, the attention mechanism will perform independent operations as follows:\n","\n","            - Base view query will attend to the base view key and value:\n","                query_base_view <- key_base_view, value_base_view\n","\n","            - upper partition query will attend to the upper partition key and value:\n","                query_upper_partition <- key_upper_partition, value_upper_partition\n","\n","            - lower partition query will attend to the lower partition key and value:\n","                query_lower_partition <- key_lower_partition, value_lower_partition\n","\n","            - Negative prompt query will attend to the negative prompt key and value:\n","                query_neg_prompt <- key_neg_prompt, value_neg_prompt\n","            \"\"\"\n","\n","            # Reshape the query, key, and value tensors:\n","            # Move the head dimension into the batch dimension for efficient attention computation\n","            # New shape: (batch_size * 2 * num_heads) x num_seqs x channels\n","            query = module.head_to_batch_dim(query)\n","            key = module.head_to_batch_dim(key)\n","            value = module.head_to_batch_dim(value)\n","\n","            # Perform attention using memory-efficient xformers\n","            hidden_states = _memory_efficient_attention_xformers(module, query, key, value)\n","\n","            # xformers may return output in fp32, so we convert it back to the original data type (matching the query tensor)\n","            hidden_states = hidden_states.to(query.dtype)\n","\n","            # Calculate the downsample rate based on the latent pixel count and the query shape\n","            downsample_rate = int((self.latent_pixels // query.shape[1]) ** 0.5)\n","\n","            height = self.latent_height // downsample_rate\n","            width = self.latent_width // downsample_rate\n","\n","            # PART 6.3 (a): Implement!\n","            # Split the hidden states into four parts: base, upper, lower, and negative prompt.\n","            (\n","                hidden_states_base,\n","                hidden_states_upper,\n","                hidden_states_lower,\n","                hidden_states_for_neg_prompt\n","            ) = # TODO: IMPLEMENT!\n","\n","            # PART 6.3 (b): Implement!\n","            # Reshape the upper hidden states into batch_size x height x width x channels_with_heads\n","            # height and width are provided, please specify batch_size and channels_with_heads dimensions.\n","            hidden_states_upper = # TODO: IMPLEMENT!\n","\n","            # PART 6.3 (c): Implement!\n","            # Reshape the lower hidden states into batch_size x height x width x channels_with_heads\n","            # height and width are provided, please specify batch_size and channels_with_heads dimensions.\n","            hidden_states_lower = # TODO: IMPLEMENT!\n","\n","            # PART 6.3 (d): Implement!\n","            # Select the upper hidden states based on the height dimension\n","            hidden_states_upper = hidden_states_upper[\n","                # TODO: IMPLEMENT!\n","            ]\n","\n","            # PART 6.3 (e): Implement!\n","            # Select the lower hidden states based on the height dimension\n","            hidden_states_lower = hidden_states_lower[\n","                # TODO: IMPLEMENT!\n","            ]\n","\n","            # PART 6.3 (f) and (g): Implement!\n","            # Concatenate the upper and lower hidden states along the height dimension and reshape the resulting tensor.\n","            # [Implement!] PART 6.3 (f) Concatenate the upper and lower hidden states along the height dimension\n","            # [Implement!] PART 6.3 (g) Reshape the concatenated tensor back into batch_size x (height * width) x channels\n","            hidden_states_controlled = # TODO: IMPLEMENT\n","\n","            hidden_states_for_pos_prompt: torch.Tensor = (\n","                hidden_states_controlled * (1. - self.base_ratio) +\n","                hidden_states_base * self.base_ratio\n","            )\n","\n","            hidden_states = torch.cat([\n","                hidden_states_for_pos_prompt,\n","                hidden_states_for_neg_prompt\n","            ])\n","\n","            # Linear projection layer\n","            hidden_states = module.to_out[0](hidden_states)\n","            # Dropout layer\n","            hidden_states = module.to_out[1](hidden_states)\n","\n","            return hidden_states\n","\n","        return forward\n","\n","    # Overwrite the forward() method for UNet's cross-attention layers\n","    def hook_forwards(self, root_module: torch.nn.Module):\n","        for name, module in root_module.named_modules():\n","            # PART 6.3 (h): Implement!\n","            # Please specify the CrossAttention Layer to be hooked.\n","            # (Hint) It is better to check the model structure of self.unet for identifying the appropriate layers.\n","\n","            if :#TODO: IMPLEMENT!\n","                module.forward = #TODO: IMPLEMENT!\n","\n","    # set random seed\n","    def set_seed(self, seed=0):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.use_deterministic_algorithms = True"]},{"cell_type":"markdown","metadata":{"id":"RCksT7QiVuqj"},"source":["**Initializing the Vertical Partitioned Image Controller**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeWieoSNVuqk"},"outputs":[],"source":["## Code Cell 6.15\n","\n","vertical_controller = VerticalPartitionedImageController(\n","                pretrained_model_name_or_path,\n","                dtype = torch.float16)"]},{"cell_type":"markdown","metadata":{"id":"kcrZ6D5oVuqk"},"source":["**Generating Images with Vertical Partitioned Prompts**\n","\n","In this example, you will generate an image with specific prompts for different partitions of the image.\n","\n","**Positive Prompt**\n","\n","The `prompt_layout` is defined as:\n","\n","```python\n","prompt_layout = [\n","    \"<base-prompt>\",               # The base prompt provides a general or global view for the entire image. This prompt influences the overall tone or theme of the image.\n","    \"<the-prompt-for-upper-partition>\",   # The prompt for the upper partition specifically influences the top side of the image.\n","    \"<the-prompt-for-lower-partition>\",  # The prompt for the lower partition specifically influences the bottom side of the image.\n","]\n","```\n","\n","- **Base Prompt**:  \n","  `<base-prompt>`  \n","  The base prompt provides a general or global view for the entire image. This prompt influences the overall tone or theme of the image.\n","\n","- **Upper Partition Prompt**:  \n","  `<the-prompt-for-upper-partition>`  \n","  This prompt specifically influences the top side of the image.\n","\n","- **Lower Partition Prompt**:  \n","  `<the-prompt-for-lower-partition>`  \n","  This prompt specifically influences the bottom side of the image.\n","\n","\n","**Image Dimensions**\n","\n","To create a taller image, we use the following dimensions:\n","\n","- **Image Height**: 1024  \n","- **Image Width**: 512"]},{"cell_type":"markdown","metadata":{"id":"p83xt4V7Vuqk"},"source":["**Drawing the Image with Partitioned Prompts**\n","\n","Now we are going to generate an image where:\n","\n","- The **upper partition** is controlled by the prompt:  \n","  \"tall mountains with snow\"\n","\n","- The **lower partition** is controlled by the prompt:  \n","  \"a yellow bus\"\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=12UKCrqISXyxSTHqzfWrToh1vvTK2BAo6\" height=\"500\"/>\n"]},{"cell_type":"markdown","source":["<font color='red'>**PART 6.4:**</font> [5 points]\n","\n","Generate images using horizontal partitioned prompts.\n","\n","<font color='red'>**Deliverables**</font>\n","\n","From *Code Cells 6.16‚Äì6.18*, generate the images and include them in **Part 6.4** of your Overleaf project"],"metadata":{"id":"rECnZGDxXv75"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_gc3qoeVuqk"},"outputs":[],"source":["## Code Cell 6.16\n","\n","prompt_layout = [\n","    \"4k, high quality\",\n","    \"tall mountains with snow\",\n","    \"a yellow bus\",\n","]\n","negative_prompt = \"worst quality, low quality, bad anatomy, jpeg artifacts\"\n","generated_images = vertical_controller(\n","            prompt_layout,\n","            negative_prompt,\n","            img_height = 1024,\n","            img_width = 512,\n","            batch_size = 1, # batch size\n","            num_inference_steps=100, # inference sampling step\n","            base_ratio=0.1, # the weight assigned to the base prompt. A value of 0 means all prompts are partition-specific, while a value of 1 means all prompts are base.\n","            guidance_scale=7.5,\n","            seed=0,\n",")\n","\n","generated_images[0].save(\"top-tallmountains_snow_right-yellowbus.png\")\n","\n","plt.imshow(generated_images[0])\n","plt.axis('off')  # Hide axes\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YtaPKStqVuqk"},"source":["**Drawing the Image with Partitioned Prompts**\n","\n","Now we are going to generate an image where:\n","\n","- The **upper partition** is controlled by the prompt:  \n","  \"big rock, lightning\"\n","\n","- The **lower partition** is controlled by the prompt:  \n","  \"horses wandering on the grassland\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHIqebGMVuqk"},"outputs":[],"source":["## Code Cell 6.17\n","\n","prompt_layout = [\n","    \"4k, high quality\",\n","    \"big rock, lightning\",\n","    \"horses wandering on the grassland\",\n","]\n","negative_prompt = \"worst quality, low quality, bad anatomy, jpeg artifacts\"\n","generated_images = vertical_controller(\n","            prompt_layout,\n","            negative_prompt,\n","            img_height = 1024,\n","            img_width = 512,\n","            batch_size = 1, # batch size\n","            num_inference_steps=100, # inference sampling step\n","            base_ratio=0.1, # the weight assigned to the base prompt. A value of 0 means all prompts are partition-specific, while a value of 1 means all prompts are base.\n","            guidance_scale=7.5,\n","            seed=0,\n",")\n","\n","plt.imshow(generated_images[0])\n","plt.axis('off')  # Hide axes\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tX6PvFg1Vuqn"},"source":["**Drawing the Image with Partitioned Prompts**\n","\n","Now we are going to generate an image where:\n","\n","- The **upper partition** is controlled by the prompt:  \n","  \"a tall house made of sturdy red bricks beneath a blue sky\"\n","\n","- The **lower partition** is controlled by the prompt:  \n","  \"a person sitting on the bench\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8igHn6WVuqn"},"outputs":[],"source":["## Code Cell 6.18\n","\n","prompt_layout = [\n","    \"4k, high quality\",\n","    \"a tall house made of sturdy red bricks beneath a blue sky\",\n","    \"a person sitting on the bench\",\n","]\n","negative_prompt = \"worst quality, low quality, bad anatomy, jpeg artifacts\"\n","generated_images = vertical_controller(\n","            prompt_layout,\n","            negative_prompt,\n","            img_height = 1024,\n","            img_width = 512,\n","            batch_size = 1, # batch size\n","            num_inference_steps=100, # inference sampling step\n","            base_ratio=0.1, # the weight assigned to the base prompt. A value of 0 means all prompts are partition-specific, while a value of 1 means all prompts are base.\n","            guidance_scale=7.5,\n","            seed=0,\n",")\n","\n","plt.imshow(generated_images[0])\n","plt.axis('off')  # Hide axes\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","source":[],"metadata":{"id":"eFXbKA1mgh3i"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"}},"nbformat":4,"nbformat_minor":0}